import pandas as pd
import cerberus
import warnings
import os
import sys
from encoded_client.encoded import ENCODED

p = os.path.dirname(os.getcwd())
sys.path.append(p)

from scripts.utils import *
from scripts.plotting import *
from scripts.mane_utils import *

warnings.filterwarnings('ignore')

configfile: 'snakemake/config.yml'

species = ['human', 'mouse']
# species = ['human']

# dl_files = ['ab', 'filt_ab', 'read_annot']
dl_files = ['ab', 'filt_ab']
events = ['A3', 'A5', 'AF', 'AL', 'MX', 'RI', 'SE']

def get_mirna_ids(config):
    df = pd.read_csv(expand(config['data']['mirna']['files'],
                species='human')[0],
                sep='\t',
                header=None)
    df.columns = ['link']

    # remove first line
    df = df.loc[~df.link.str.contains('metadata')]

    df['id'] = df.link.str.rsplit('/', expand=True, n=1)[1]
    df['id'] = df['id'].str.split('.tsv', expand=True, n=1)[0]
    return df['id'].tolist()

# def get_lr_ids(file_format='bam', species='human'):
#     metadata = get_lr_exp_meta(species)
#     if file_format == 'bam':
#         metadata = metadata.loc[metadata.output_type=='unfiltered alignments']
#     elif file_format == 'label_bam':
#         metadata = metadata.loc[metadata.output_type=='alignments']
#     elif file_format == 'fastq':
#         metadata = metadata.loc[metadata.output_type=='reads']
#     return metadata.file.tolist()

# def get_df_lr_ids(file_id_df, file_format, species):
#     df = file_id_df.copy(deep=True)
#     if file_format == 'bam':
#         df = df.loc[df.output_type=='unfiltered alignments']
#     elif file_format == 'label_bam':
#         df = df.loc[df.output_type=='alignments']
#     elif file_format == 'fastq':
#         df = df.loc[df.output_type=='reads']
#     if species:
#         df = df.loc[df.species==species]
#     ids = df.file.tolist()
#     return ids

# encode lr meta
def get_meta_df():
    meta_df = pd.DataFrame()
    for f, s in zip(list(expand(config['data']['meta'], species=species)), species):
        temp = pd.read_csv(f, sep='\t')
        temp['species'] = s
        meta_df = pd.concat([meta_df, temp], axis=0)
    # if meta_df.dataset.duplicated.any():
    #     raise ValueError('Mouse and human dataset names not unique')
    return meta_df
meta_df = get_meta_df()

wildcard_constraints:
    dataset= '|'.join([re.escape(x) for x in meta_df.dataset.tolist()]),

def get_col_from_meta_df(wc, col):
    temp = meta_df.copy(deep=True)
    if wc.species:
        temp = meta_df.loc[meta_df.species == wc.species]
    return temp[col].tolist()


def get_encid_from_dataset(dataset, file_format):
    m = {'label_bam': 'ENCODE_alignments_id',
         'bam': 'ENCODE_unfiltered_alignments_id',
         'fastq': 'ENCODE_reads_id'}
    id = meta_df.loc[meta_df.dataset == dataset, m[file_format]].values[0]
    return id




# meta = pd.read_csv('ref/lr_file_ids.tsv', sep='\t')

# encode procap meta
# procap_meta = pd.read_csv('data/human/procap/metadata.tsv')

rule all:
    input:
        # expand(config['data']['ab'], species=species),
        # expand(config['data']['talon_filt_ab'], species=species),
        # expand(config['data']['lapa_filt_ab'], species=species),
        # expand(config['data']['lapa_gtf'], species=species),
        # expand(config['data']['filt_ab'], species=species),
        # expand(config['data']['cerb_annot'], species=species),
        # expand(config['data']['cerb_gtf'], species=species),
        # expand(config['ref']['new_gencode_gtf'], species=species),
        # expand(config['ref']['cerberus']['gtf'], species=species),
        # expand(config['ref']['cerberus']['t_info'], species=species),
        # expand(config['ref']['cerberus']['g_info'], species=species), #,
        # expand(config['data']['t_info'], species=species),
        # expand(config['data']['major_isos'], species=species, obs_col='sample'),
        # expand(config['data']['exp_gene_subset'], species='human', obs_col=['sample', 'dataset']),
        # expand(config['data']['exp_gene_subset'], species='mouse', obs_col=['sample']),
        # expand(config['data']['pi_tpm']['tss'], species='human', obs_col=['sample', 'dataset']),
        # expand(config['data']['pi_tpm']['ic'], species='human', obs_col=['sample', 'dataset']),
        # expand(config['data']['pi_tpm']['tes'], species='human', obs_col=['sample', 'dataset']),
        # expand(config['data']['pi_tpm']['triplet'], species='human', obs_col=['sample', 'dataset']),
        # expand(config['data']['pi_tpm']['triplet'], species='mouse', obs_col=['sample']),
        # expand(config['ref']['t_info'], species=species),
        # expand(config['ref']['g_info'], species=species),
        # expand(config['data']['sg'], species=species),
        # expand(config['data']['mirna']['tsv'], species='human', encid=get_mirna_ids(config)),
        # expand(config['data']['bam'], species='human', encid=get_lr_ids('bam', species='human')),
        # expand(config['data']['fastq_gz'], species='human', encid=get_lr_ids('fastq', species='human')),
        # expand(config['data']['read_len_meta'], species='human'),
        # expand(config['data']['suppa']['psi'],
        #         event=events,
        #         species='human'),
        # expand(config['data']['suppa']['gtex']['events']['A3'],
        #                 species='human'),
        # expand(config['data']['suppa']['gtex']['cerb_ids'],
        #        species='human'),
        # expand(config['data']['psi'],
        #        species='human'),
        # expand(config['data']['suppa']['matching_events'],
        #        species='human',
        #        event=['AF', 'AL'])
        # expand(config['data']['p_pred']['summary'], species=species),
        # expand(config['data']['p_pred']['blastp_6'], species=species)
        config['data']['spikes']['talon_config']
        # config['data']['spikes']['ab'],
        # config['data']['spikes']['gene_ab'],
        # config['data']['spikes']['filt_ab'],
        # config['data']['spikes']['filt_gtf'],


rule dl:
    resources:
        mem_gb = 4,
        threads = 1
    shell:
        "wget -O {output.out} {params.link}"

rule gunzip:
    resources:
        mem_gb = 4,
        threads = 1
    shell:
        "gunzip -c {input.gz} > {output.out}"

################################################################################
########################### Data download ######################################
################################################################################

use rule dl as dl_ab with:
  params:
    link = lambda w:config['encode'][w.species]['ab']
  output:
    out = config['data']['ab']

use rule dl as dl_filt_ab with:
  params:
    link = lambda w:config['encode'][w.species]['filt_ab']
  output:
    out = config['data']['filt_ab']

use rule dl as dl_read_annot with:
  params:
    link = lambda w:config['encode'][w.species]['read_annot']
  output:
    out = config['data']['read_annot']

use rule dl as dl_cerb_annot with:
  params:
    link = lambda w:config['encode'][w.species]['cerb_annot']
  output:
    out = config['data']['cerb_annot']

use rule dl as dl_ref_genome with:
  params:
    link = lambda w:config['encode'][w.species]['fa_gz']
  output:
    out = config['ref']['fa_gz']

use rule gunzip as gunzip_ref_genome with:
  input:
    gz = config['ref']['fa_gz']
  output:
    out = config['ref']['fa']

use rule dl as dl_gencode_gtf with:
  params:
    link = lambda w:config['encode'][w.species]['new_gencode_gtf_gz']
  output:
    out = config['ref']['new_gencode_gtf_gz']

use rule gunzip as gunzip_gencode with:
  input:
    gz = config['ref']['new_gencode_gtf_gz']
  output:
    out = config['ref']['new_gencode_gtf']

use rule dl as dl_gencode_fa with:
  params:
    link = lambda w:config['link'][w.species]['p_fa']
  output:
    out = config['ref']['p_fa_gz']

use rule gunzip as gunzip_gencode_fa with:
  input:
    gz = config['ref']['p_fa_gz']
  output:
    out = config['ref']['p_fa']

rule fix_gencode_fa_headers:
    input:
        fa = config['ref']['p_fa']
    resources:
        threads = 1,
        mem_gb = 4
    output:
        fa = config['ref']['p_fa_header']
    run:
        fix_fa_headers(input.fa, output.fa)

use rule dl as dl_cerb_gtf with:
  params:
    link = lambda w:config['encode'][w.species]['cerb_gtf_gz']
  output:
    out = config['data']['cerb_gtf_gz']

use rule gunzip as gunzip_cerb with:
  input:
    gz = config['data']['cerb_gtf_gz']
  output:
    out = config['data']['cerb_gtf']

use rule dl as dl_lapa_gtf with:
  params:
    link = lambda w:config['encode'][w.species]['lapa_gtf_gz']
  output:
    out = config['data']['lapa_gtf_gz']

use rule gunzip as gunzip_lapa with:
  input:
    gz = config['data']['lapa_gtf_gz']
  output:
    out = config['data']['lapa_gtf']

use rule dl as dl_talon_filt_ab with:
  params:
    link = lambda w:config['encode'][w.species]['talon_filt_ab']
  output:
    out = config['data']['talon_filt_ab']

use rule dl as dl_lapa_filt_ab with:
  params:
    link = lambda w:config['encode'][w.species]['lapa_filt_ab']
  output:
    out = config['data']['lapa_filt_ab']

rule get_chrom_sizes:
    resources:
        threads = 1,
        mem_gb = 8
    shell:
        """
        faidx {input.fa} -i chromsizes > {output.chrom_sizes}
        """

################################################################################
################################# Minimap2 #####################################
################################################################################
rule sam_to_bam:
    shell:
        """
        samtools view -Sb {input.sam} > {output.bam}
        """

rule minimap2:
    resources:
        threads = 32,
        mem_gb = 64
    shell:
        """
        module load minimap2
        minimap2 \
            -t {resources.threads} \
            -ax splice \
            -uf \
            --secondary=no \
            -C5 \
            {input.fa} \
            {input.fastq} \
            > {output.sam}
            """

################################################################################
######################### Save list of file ids for LR #########################
################################################################################
rule get_lr_file_ids:
    resources:
        mem_gb = 16,
        threads = 1
    run:
        df = pd.DataFrame()
        df['file_id'] = params.encids
        df.to_csv(output.ids, sep='\t', index=False)

# use rule get_lr_file_ids as rule get_label_bam_lr_file_ids with:
#     params:
#         encids = lambda wc:get_lr_ids('label_bam', species=wc.species)
#     output:
#         ids = config['data']['label_bam']['ids']
#
# use rule get_lr_file_ids as rule get_bam_lr_file_ids with:
#     params:
#         encids = lambda wc:get_lr_ids('bam', species=wc.species)
#     output:
#         ids = config['data']['bam']['ids']
#
# use rule get_lr_file_ids as rule get_fastq_lr_file_ids with:
#     params:
#         encids = lambda wc:get_lr_ids('fastq', species=wc.species)
#     output:
#         ids = config['data']['fastq']['ids']

################################################################################
######################### Spike download / processing ##########################
################################################################################

use rule dl as dl_sirv_gtf with:
  params:
    link = config['encode']['spikes']['sirv_gtf_gz']
  output:
    out = config['ref']['spikes']['sirv_gtf_gz']

use rule gunzip as gunzip_sirv_gtf with:
  input:
    gz = config['ref']['spikes']['sirv_gtf_gz']
  output:
    out = config['ref']['spikes']['sirv_gtf']

use rule dl as dl_sirv_fa with:
  params:
    link = config['encode']['spikes']['sirv_fa_gz']
  output:
    out = config['ref']['spikes']['sirv_fa_gz']

use rule gunzip as gunzip_sirv_fa with:
  input:
    gz = config['ref']['spikes']['sirv_fa_gz']
  output:
    out = config['ref']['spikes']['sirv_fa']

use rule dl as dl_ercc_fa with:
  params:
    link = config['encode']['spikes']['ercc_fa_gz']
  output:
    out = config['ref']['spikes']['ercc_fa_gz']

use rule gunzip as gunzip_ercc_fa with:
  input:
    gz = config['ref']['spikes']['ercc_fa_gz']
  output:
    out = config['ref']['spikes']['ercc_fa']

rule get_ercc_gtf:
    input:
        fa = config['ref']['spikes']['ercc_fa']
    resources:
        mem_gb = 16,
        threads = 1
    output:
        gtf = config['ref']['spikes']['ercc_gtf']
    shell:
        """
        python ../scripts/merge_encode_annotations.py \
            -o temp_reformat_ercc.gtf \
            {input.fa}

        python ../scripts/reformat_gtf.py \
            -o {output.gtf} \
            -gtf temp_reformat_ercc.gtf

        rm temp_reformat_ercc.gtf
        """

rule get_spike_gtf:
    input:
        sirv = config['ref']['spikes']['sirv_gtf'],
        ercc = config['ref']['spikes']['ercc_gtf']
    resources:
        mem_gb = 16,
        threads = 2
    output:
        all = config['ref']['spikes']['spike_gtf']
    shell:
        """
        cat {input.sirv} > {output.all}
        cat {input.ercc} >> {output.all}
        """

rule get_spike_fa:
    input:
        sirv = config['ref']['spikes']['sirv_fa'],
        ercc = config['ref']['spikes']['ercc_fa']
    resources:
        threads = 1,
        mem_gb = 4
    output:
        all = config['ref']['spikes']['spike_fa']
    shell:
        """
        cat {input.sirv} > {output.all}
        cat {input.ercc} >> {output.all}
        """

use rule get_chrom_sizes as get_spike_chrom_sizes with:
    input:
        fa = config['ref']['spikes']['spike_fa']
    output:
        chrom_sizes = config['ref']['spikes']['chrom_sizes']

rule get_spike_chrom_bed:
    input:
        chrom_sizes = config['ref']['spikes']['chrom_sizes']
    resources:
        mem_gb = 16,
        threads = 1
    output:
        bed = config['ref']['spikes']['chrom_bed']
    run:
        df = pd.read_csv(input.chrom_sizes,
                         sep='\t',
                         header=None,
                         names=['chr', 'stop'])
        df['start'] = 0
        df = df[['chr', 'start', 'stop']]
        df.to_csv(output.bed,
                  sep='\t',
                  header=None,
                  index=False)

################################################################################
############################# Data processing ##################################
################################################################################
def get_new_gencode_source(wildcards):
    if wildcards.species == 'human':
        source = 'v40'
    elif wildcards.species == 'mouse':
        source = 'vM25'
    return source

# annotate gencode gtf w/ cerberus
rule cerberus_update_gtf:
    input:
        annot = config['data']['cerb_annot'],
        gtf = config['ref']['new_gencode_gtf']
    output:
        update_gtf = config['ref']['cerberus']['gtf']
    resources:
        mem_gb = 56,
        threads = 1
    params:
        source = get_new_gencode_source
    shell:
        "cerberus replace_gtf_ids \
            --h5 {input.annot} \
            --gtf {input.gtf} \
            --source {params.source} \
            --update_ends \
            --collapse \
            -o {output.update_gtf}"

# get transcript info
rule get_t_info:
    resources:
        mem_gb = 56,
        threads = 1
    run:
        get_transcript_info(input.gtf, output.o)

use rule get_t_info as ref_cerb_t_info with:
    input:
        gtf = config['ref']['cerberus']['gtf']
    output:
        o = config['ref']['cerberus']['t_info']

use rule get_t_info as ref_t_info with:
    input:
        gtf = config['ref']['new_gencode_gtf']
    output:
        o = config['ref']['t_info']

use rule get_t_info as cerb_t_info with:
    input:
        gtf = config['data']['cerb_gtf']
    output:
        o = config['data']['t_info']

# get gene info
rule get_g_info:
    resources:
        mem_gb = 56,
        threads = 1
    run:
        get_gene_info(input.gtf, output.o)

use rule get_g_info as cerb_ref_g_info with:
    input:
        gtf = config['ref']['cerberus']['gtf']
    output:
        o = config['ref']['cerberus']['g_info']

use rule get_g_info as ref_g_info with:
    input:
        gtf = config['ref']['new_gencode_gtf']
    output:
        o = config['ref']['g_info']

################################################################################
################################## Swan ########################################
################################################################################
def make_sg(input, params, wildcards):

    # initialize
    sg = swan.SwanGraph()
    sg.add_annotation(input.annot)
    sg.add_transcriptome(input.gtf, include_isms=True)
    sg.add_abundance(input.ab)
    sg.add_abundance(input.gene_ab, how='gene')
    sg.save_graph(params.prefix)

    # add metadata and add colors
    sg.add_metadata(input.meta)
    c_dict, order = get_biosample_colors(wildcards.species)
    sg.set_metadata_colors('sample', c_dict)

    # human only settings
    if wildcards.species == 'human':
        c_dict, order = get_ad_colors()
        sg.set_metadata_colors('health_status', c_dict)
    # save
    sg.save_graph(params.prefix)

rule swan_init:
    input:
        annot = config['ref']['cerberus']['gtf'],
        ab = config['data']['filt_ab'],
        gene_ab = config['data']['ab'],
        gtf = config['data']['cerb_gtf'],
        meta = config['data']['meta']
    params:
        prefix = config['data']['sg'].replace('.p', '')
    resources:
        mem_gb = 64,
        threads = 1
    output:
        sg = config['data']['sg']
    run:
        make_sg(input, params, wildcards)

rule major_isos:
    input:
        sg = config['data']['sg'],
        filt_ab = config['data']['filt_ab']
    resources:
        mem_gb = 16,
        threads = 8
    params:
        min_tpm = 1,
        gene_subset = 'polya'
    output:
        ofile = config['data']['major_isos']
    run:
        get_major_isos(input.sg,
                       input.filt_ab,
                       wildcards.obs_col,
                       output.ofile,
                       min_tpm=params.min_tpm,
                       gene_subset=params.gene_subset)

################################################################################
################################## MANE ########################################
################################################################################
rule get_exp_genes:
    input:
        ab = config['data']['ab']
    resources:
        mem_gb = 16,
        threads = 8
    params:
        min_tpm = 1,
        gene_subset = 'polya',
    output:
        ofile = config['data']['exp_gene_subset']
    run:
        get_exp_gene_subset(input.ab,
                            params.min_tpm,
                            wildcards.obs_col,
                            output.ofile,
                            wildcards.species)

rule get_pi_tpm:
    input:
        swan_file = config['data']['sg']
    resources:
        mem_gb = 32,
        threads = 8
    params:
        odir = config['data']['pi_tpm']['tss'].rsplit('/', maxsplit=1)[0]
    output:
        config['data']['pi_tpm']['tss'],
        config['data']['pi_tpm']['ic'],
        config['data']['pi_tpm']['tes'],
        config['data']['pi_tpm']['triplet']
    run:
        sg = swan.read(input.swan_file)
        get_pi_tpm_tables(sg, wildcards.obs_col, params.odir)

################################################################################
################################## miRNA #######################################
################################################################################

rule dl_mirna:
    resources:
        mem_gb = 32,
        threads = 8
    output:
        tsv = config['data']['mirna']['tsv']
    shell:
        "wget https://www.encodeproject.org/files/{wildcards.encid}/@@download/{wildcards.encid}.tsv -O {output.tsv}"

################################################################################
################################## Procap ######################################
################################################################################
rule dl_procap:
    resources:
        mem_gb = 32,
        threads = 1
    output:
        bed = config['procap']['bed_gz']
    shell:
        "wget https://www.encodeproject.org/files/{wildcards.encid}/@@download/{wildcards.encid}.tsv -O {output.tsv}"

################################################################################
############################## Diane's stuff ###################################
################################################################################
rule dl_lr_bam:
    resources:
        mem_gb = 32,
        threads = 8
    output:
        bam = temporary(config['data']['bam'])
    shell:
        "wget https://www.encodeproject.org/files/{wildcards.encid}/@@download/{wildcards.encid}.bam -O {output.bam}"

rule dl_lr_label_bam:
    resources:
        mem_gb = 32,
        threads = 8
    output:
        bam = temporary(config['data']['label_bam'])
    shell:
        "wget https://www.encodeproject.org/files/{wildcards.encid}/@@download/{wildcards.encid}.bam -O {output.bam}"


rule dl_lr_fastq:
    resources:
        mem_gb = 32,
        threads = 8
    params:
        encid = lambda w:get_encid_from_dataset(w.dataset, 'fastq')
    output:
        fastq = config['data']['fastq_gz']
    shell:
        "wget https://www.encodeproject.org/files/{params.encid}/@@download/{wildcards.encid}.fastq.gz -O {output.fastq}"

rule get_lr_read_lens:
    input:
        bams = expand(config['data']['bam'],
                      species='human',
                      dataset=lambda w:get_col_from_meta_df(wc, col='dataset')),
        fastqs = expand(config['data']['fastq_gz'],
                        species='human',
                        dataset=lambda w:get_col_from_meta_df(wc, col='dataset'))
    resources:
        mem_gb = 32,
        threads = 8
    output:
        tsv = config['data']['read_len_meta']
    run:
        get_lr_read_lens(input.bams, input.fastqs, output.tsv)

################################################################################
################################# SUPPA ########################################
################################################################################

use rule dl as dl_gtex_gtf with:
  params:
    link = lambda w:config['encode'][w.species]['gtex_gtf_gz']
  output:
    out = config['ref']['gtex_gtf_gz']

use rule gunzip as gunzip_gtex with:
  input:
    gz = config['ref']['gtex_gtf_gz']
  output:
    out = config['ref']['gtex_gtf']

rule get_transcript_novelties:
    input:
        annot = config['data']['cerb_annot'],
        filt_ab = config['data']['filt_ab'],
        t_meta = config['data']['t_info']
    params:
        min_tpm = 1,
        gene_subset = 'polya',
        ver = 'v40_cereberus'
    resources:
        mem_gb = 8,
        threads = 1
    output:
        novelties = config['data']['novelties']
    run:
        get_transcript_novelties(input.annot,
                                 input.filt_ab,
                                 input.t_meta,
                                 params.min_tpm,
                                 params.gene_subset,
                                 params.ver,
                                 output.novelties)

rule preproc_suppa:
    input:
        gtf = config['data']['cerb_gtf'],
        nov = config['data']['novelties'],
        filt_ab = config['data']['filt_ab']
    params:
        temp_filt_ab_nov = 'temp_filt_ab_nov.tsv',
        temp_filt_ab_nov_2 = 'temp_filt_ab_nov_2.tsv'
    output:
        filt_ab = config['data']['suppa']['filt_ab'],
        gtf = config['data']['suppa']['gtf']
    conda:
        'seurat'
    shell:
        """
        sed 's/,/_/g' {input.gtf} > {output.gtf}
        Rscript ../scripts/filter_abundance_mat_by_novelty.R \
            --novelties {input.nov} \
            --filtab {input.filt_ab} \
            --ofile {params.temp_filt_ab_nov}
        cut -f4,12- {params.temp_filt_ab_nov} | sed 's/,/_/g' > {params.temp_filt_ab_nov_2}
        python ../scripts/suppa_format_rm_colname.py {params.temp_filt_ab_nov_2}
        tail -c +2 {params.temp_filt_ab_nov_2} > {output.filt_ab}
        """

rule suppa_generate_events:
    resources:
        mem_gb = 16,
        threads = 1
    shell:
        """
        suppa.py generateEvents \
            -i {input.gtf} \
            -o {params.opref} \
            -f ioe \
            -e SE SS MX RI FL
        """

use rule suppa_generate_events as ge_cerb with:
    input:
        gtf = config['data']['suppa']['gtf']
    params:
        opref = config['data']['suppa']['events']['A3'].rsplit('_', maxsplit=2)[0]
        # opref = config['data']['suppa']['events']
    output:
        config['data']['suppa']['events']['A3'],
        config['data']['suppa']['events']['A5'],
        config['data']['suppa']['events']['AF'],
        config['data']['suppa']['events']['AL'],
        config['data']['suppa']['events']['MX'],
        config['data']['suppa']['events']['RI'],
        config['data']['suppa']['events']['SE'],

use rule suppa_generate_events as ge_gtex with:
    input:
        gtf = config['ref']['gtex_gtf']
    params:
        opref = config['data']['suppa']['gtex']['events']['A3'].rsplit('_', maxsplit=2)[0]
    output:
        config['data']['suppa']['gtex']['events']['A3'],
        config['data']['suppa']['gtex']['events']['A5'],
        config['data']['suppa']['gtex']['events']['AF'],
        config['data']['suppa']['gtex']['events']['AL'],
        config['data']['suppa']['gtex']['events']['MX'],
        config['data']['suppa']['gtex']['events']['RI'],
        config['data']['suppa']['gtex']['events']['SE']

rule suppa_psi:
    resources:
        mem_gb = 16,
        threads = 1
    shell:
        """
        suppa.py psiPerEvent \
            --ioe-file {input.ioe} \
            --expression-file {input.filt_ab} \
            --o {params.opref}
        """

use rule suppa_psi as psi_cerb with:
    input:
        ioe = lambda w:config['data']['suppa']['events'][w.event],
        filt_ab = config['data']['suppa']['filt_ab']
    params:
        opref = config['data']['suppa']['psi'].rsplit('.psi', maxsplit=1)[0]
    output:
        out = config['data']['suppa']['psi']

rule get_gtex_cerb_ids:
    input:
        cerb_annot = config['data']['cerb_annot']
    resources:
        mem_gb = 2,
        threads = 1
    output:
        cerb_ids = config['data']['suppa']['gtex']['cerb_ids']
    run:
        get_gtex_cerberus_ids(input.cerb_annot, output.cerb_ids)


rule get_cerberus_psi:
    input:
        filt_ab = config['data']['filt_ab']
    params:
        min_tpm = 1,
        gene_subset = 'polya'
    resources:
        threads = 1,
        mem_gb = 64
    output:
        ofile = config['data']['psi']
    run:
        get_cerberus_psi(input.filt_ab,
                         params.min_tpm,
                         params.gene_subset,
                         output.ofile)

e_map = {'tss': 'AF',
      'tes': 'AL',
      'AL': 'tes',
      'AF': 'tss'}
rule get_matching_events:
    input:
        cerb_file = config['data']['psi'],
        suppa_file = config['data']['suppa']['psi'],
        lib_meta = config['data']['meta']
    params:
        kind = lambda w:e_map[w.event]
    output:
        ofile = config['data']['suppa']['matching_events']
    run:
        get_cerb_suppa_matching_events(input.cerb_file,
                                       input.suppa_file,
                                       output.ofile,
                                       input.lib_meta,
                                       params.kind)



# use rule get_matching_events as gme_al with:
#
#
# use rule get_matching_events as gme_al with:
#     input:
#         cerb_file = config['data']['psi'],
#         suppa_file = config['data']['suppa']['psi'],
#         lib_meta = config['data']['meta'],
#     output:
#         ofile = config['data']['suppa']['matching_events']

################################################################################
######################### Milad's TSS prediction ###############################
################################################################################

# TODO

# automate downloading the bigwigs, getting the processed tss data from cerberus
# and lapa, and downloading the jamboree tss files

################################################################################
##################### Narges's short-read data proc ############################
################################################################################

# TODO

################################################################################
######################## Sam's gene expression #################################
################################################################################

# TODO

################################################################################
################################ Protein prediction ############################
################################################################################

rule tama_gtf_to_bed:
    input:
        gtf = config['data']['cerb_gtf']
    params:
        tama_dir = config['tama_dir']
    resources:
        threads = 4,
        mem_gb = 32
    output:
        bed = config['data']['p_pred']['gtf_bed']
    shell:
        """
        python {params.tama_dir}/format_converter/tama_format_gtf_to_bed12_ensembl.py \
          {input.gtf} \
          {output.bed}
        """

rule bed_to_fasta:
    input:
        bed = config['data']['p_pred']['gtf_bed'],
        ref = config['ref']['fa']
    output:
        fa = config['data']['p_pred']['fa']
    resources:
        threads = 4,
        mem_gb = 32
    shell:
        """
        # module load bedtools2
        bedtools getfasta \
            -name \
            -split \
            -s \
            -fi {input.ref} \
            -bed {input.bed} \
            -fo {output.fa}
        """

rule tama_orf:
    input:
        fa = config['data']['p_pred']['fa']
    params:
        tama_dir = config['tama_dir']
    resources:
        threads = 4,
        mem_gb = 32
    output:
        orf_fa = config['data']['p_pred']['orf_fa']
    shell:
        """
        python {params.tama_dir}/orf_nmd_predictions/tama_orf_seeker.py \
            -f {input.fa} \
            -o {output.orf_fa}
        """

rule make_blastp_db:
    input:
        fa = config['ref']['p_fa_header']
    resources:
        mem_gb = 64,
        threads = 32
    params:
        opref = config['ref']['p_db'].replace('.pdb', '')

    output:
        db = config['ref']['p_db']
    shell:
        """
        makeblastdb \
            -in {input.fa} \
             -dbtype prot \
             -parse_seqids \
             -out {params.opref}
        """

# rule blast:
#     input:
#         orf_fa = config['data']['p_pred']['orf_fa'],
#         db = config['ref']['p_db']
#     params:
#         opref = config['ref']['p_db'].replace('.pdb', '')
#     resources:
#         mem_gb = 32,
#         threads = 32
#     output:
#         out = config['data']['p_pred']['blastp']
#     shell:
#         """
#          blastp \
#           -evalue 1e-10 \
#           -num_threads 32 \
#           -db {params.opref} \
#           -outfmt 0 \
#           -query {input.orf_fa} > \
#           {output.out}
#         """

rule blast_6:
    input:
        orf_fa = config['data']['p_pred']['orf_fa'],
        db = config['ref']['p_db']
    params:
        opref = config['ref']['p_db'].replace('.pdb', '')
    resources:
        mem_gb = 32,
        threads = 32
    output:
        out = config['data']['p_pred']['blastp_6']
    shell:
        """
         blastp \
          -evalue 1e-10 \
          -num_threads 32 \
          -db {params.opref} \
          -outfmt "6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore slen qlen" \
          -query {input.orf_fa} > \
          {output.out}
        """

# rule parse_blastp:
#     input:
#         blastp = config['data']['p_pred']['blastp_6']
#     params:
#         tama_dir = config['tama_dir']
#     resources:
#         mem_gb = 32,
#         threads = 1
#     output:
#         parsed = config['data']['p_pred']['blastp_parsed']
#     shell:
#         """
#         python {params.tama_dir}/orf_nmd_predictions/tama_orf_blastp_parser.py \
#           -b {input.blastp} \
#           -o {output.parsed}
#         """

rule parse_blastp:
    input:
        blastp = config['data']['p_pred']['blastp_6'],
        fa = config['data']['p_pred']['orf_fa']
    resources:
        mem_gb = 32,
        threads = 1
    output:
        parsed = config['data']['p_pred']['blastp_parsed']
    run:
        parse_blastp(input.blastp, input.fa, output.parsed)


rule tama_cds_bed_regions:
    input:
        parsed = config['data']['p_pred']['blastp_parsed'],
        gtf_bed = config['data']['p_pred']['gtf_bed'],
        fa = config['data']['p_pred']['fa']
    params:
        tama_dir = config['tama_dir']
    resources:
        mem_gb = 4,
        threads = 1
    output:
        bed = config['data']['p_pred']['cds_bed']
    shell:
        """
        python {params.tama_dir}/orf_nmd_predictions/tama_cds_regions_bed_add.py \
            -p {input.parsed} \
            -a {input.gtf_bed} \
            -f {input.fa} \
            -o {output.bed}
        """

rule get_pp_summary:
    input:
        fa = config['data']['p_pred']['orf_fa'],
        parsed = config['data']['p_pred']['blastp_parsed'],
        cds = config['data']['p_pred']['cds_bed']
    resources:
        mem_gb = 4,
        threads = 1
    output:
        summary = config['data']['p_pred']['summary']
    run:
        get_pp_info(input.fa,
                    input.cds,
                    input.parsed,
                    output.summary)

################################################################################
############################# TALON utilities ##################################
################################################################################
rule talon_label:
    resources:
        mem_gb = 16,
        threads = 1
    shell:
        """
        talon_label_reads \
            --f {input.bam} \
            --g {input.fa} \
            --tmpDir {params.opref} \
            --ar 20  \
            --deleteTmp  \
            --o {params.opref}
        """

def get_spike_talon_run_files():
    files = expand(config['data']['spikes']['bam_label'],
                   species=meta_df.species.tolist()[:5],
                   dataset=meta_df.dataset.tolist()[:5])
    return files

rule talon_config:
    input:
        files = get_spike_talon_run_files(),
        meta = expand(config['data']['meta'], species=species)
    resources:
        threads = 1,
        mem_gb = 1
    output:
        config = config['data']['spikes']['talon_config']
    run:
        config = pd.DataFrame()
        config['file'] = list(input.files)
        config['file_base'] = config.file.str.rsplit('/', n=1, expand=True)[1]
        config['file_base'] = config.file_base.str.rsplit('.bam', n=1, expand=True)[0]
        config['platform'] = 'PacBio'
        # import pdb; pdb.set_trace()

        # fix this -- use meta_df; make sure to append species to dataset name as well

        # # get the dataset / sample IDs from the metadata file
        # df = pd.DataFrame()
        # for meta_file in list(input.meta):
        #     temp = pd.read_csv(meta_file, sep='\t')
        #     df = pd.concat([df, temp], axis=0)
        #
        # df = df[['ENCODE_file_id', 'dataset']].rename({'ENCODE_file_id': 'file_base'}, axis=1)
        # df = df[['file_base', 'dataset']]
        #
        # # import pdb;pdb.set_trace()
        # config = config.merge(df, how='left', on='file_base')
        #
        # config['sample'] = config['dataset']
        #
        # config = config[['dataset', 'sample', 'platform', 'file']]
        #
        # config.to_csv(output.config, header=None, sep=',', index=False)

rule talon_init:
	input:
		ref_gtf = config['ref']['spikes']['spike_gtf']
	output:
		db = config['ref']['spikes']['talon_db']
	params:
		talon_opref = config['ref']['spikes']['talon_db'].rsplit('.db', maxsplit=1)[0],
		genome = 'spikes',
		annot = 'spikes',
		idprefix = 'spikes'
	resources:
		mem_gb = 32,
		threads = 16
	shell:
		'talon_initialize_database \
    		--f {input.ref_gtf} \
    		--g {params.genome} \
    		--a {params.annot} \
    		--l 0 \
    		--idprefix {params.idprefix} \
    		--5p 500 \
    		--3p 300 \
    		--o {params.talon_opref}'

rule talon:
    input:
        ref = config['ref']['spikes']['talon_db'],
        config = config['data']['spikes']['talon_config']
    params:
        genome = 'spikes',
        opref = config['data']['spikes']['talon_db'].rsplit('_talon', maxsplit=1)[0],
    resources:
        mem_gb = 256,
        threads = 30
    output:
        db = config['data']['spikes']['talon_db'],
        annot = config['data']['spikes']['read_annot']
    shell:
        """
        ref_db={input.ref}_back
        cp {input.ref} ${{ref_db}}
        talon \
            --f {input.config} \
            --db ${{ref_db}} \
            --build {params.genome} \
            --tmpDir {params.opref}_temp/ \
            --threads {resources.threads} \
            --o {params.opref}
        mv ${{ref_db}} {params.opref}_talon.db
        """

rule talon_unfilt_ab:
    input:
        db = config['data']['spikes']['talon_db']
    resources:
        threads = 1,
        mem_gb = 32
    params:
        genome = 'spikes',
        annot = 'spikes',
        opref = config['data']['spikes']['ab'].rsplit('_talon', maxsplit=1)[0]
    output:
        ab = config['data']['spikes']['ab']
    shell:
        """
        talon_abundance \
            --db {input.db} \
            -a {params.annot} \
            -b {params.genome} \
            --o {params.opref}
        """

rule talon_filt:
    input:
        db = config['data']['spikes']['talon_db']
    resources:
        threads = 1,
        mem_gb = 128
    params:
        annot = 'spikes'
    output:
        list = config['data']['spikes']['filt_list']
    shell:
        """
        talon_filter_transcripts \
            --db {input.db} \
            -a {params.annot} \
            --maxFracA=0.5 \
            --minCount=5 \
            --minDatasets=2 \
            --o {output.list}
        """

rule talon_filt_ab:
    input:
        db = config['data']['spikes']['talon_db'],
        filt = config['data']['spikes']['filt_list']
    resources:
        threads = 1,
        mem_gb = 128
    params:
        genome = 'spikes',
        annot = 'spikes',
        opref = config['data']['spikes']['filt_ab'].rsplit('_talon', maxsplit=1)[0]
    output:
        filt_ab = config['data']['spikes']['filt_ab']
    shell:
        """
        talon_abundance \
            --db {input.db} \
            -a {params.annot} \
            -b {params.genome} \
            --whitelist {input.filt} \
            --o {params.opref}
        """

rule talon_gtf:
    input:
        db = config['data']['spikes']['talon_db'],
        filt = config['data']['spikes']['filt_list']
    resources:
        threads = 1,
        mem_gb = 128
    params:
        genome = 'spikes',
        annot = 'spikes',
        opref = config['data']['spikes']['filt_gtf'].rsplit('_talon', maxsplit=1)[0]
    output:
        gtf = config['data']['spikes']['filt_gtf']
    shell:
        """
        talon_create_GTF \
            --db {input.db} \
            -b {params.genome} \
            -a {params.annot} \
            --whitelist {input.filt} \
            --observed \
            --o {params.opref}
        """

rule talon_gene_counts:
    input:
        ab = config['data']['spikes']['ab']
    output:
        gene_ab = config['data']['spikes']['gene_ab']
    resources:
        mem_gb = 64,
        threads = 1
    run:
        get_gene_counts_matrix(input.ab, output.gene_ab)

################################################################################
############################## Spike data proc #################################
################################################################################
use rule minimap2 as map_spikes with:
    input:
        fastq = config['data']['fastq_gz'],
        fa = config['ref']['spikes']['spike_fa']
    output:
        sam = temporary(config['data']['minimap']['sam'])

use rule sam_to_bam as sb_spikes with:
    input:
        sam = config['data']['minimap']['sam']
    output:
        bam = config['data']['minimap']['bam']

rule get_spike_reads:
    input:
        bam = config['data']['minimap']['bam'],
        spike_bed = config['ref']['spikes']['chrom_bed']
    conda:
        'samtools'
    resources:
        mem_gb = 32,
        threads = 8
    output:
        bam = config['data']['spikes']['bam']
    shell:
        """
        samtools index {input.bam}
        samtools view -bh --regions-file {input.spike_bed} {input.bam} > {output.bam}
        """

use rule talon_label as talon_label_spike with:
        input:
            bam = config['data']['minimap']['sam'],
            fa = config['ref']['spikes']['spike_fa']
        params:
            a_range = 20,
            opref = lambda w:expand(config['data']['spikes']['sam_label'],
                                    zip,
                                    species=w.species,
                                    dataset=get_col_from_meta_df(w, 'dataset'))[0].rsplit('_labeled.sam', maxsplit=1)[0]
        output:
            bam = config['data']['spikes']['sam_label']

use rule sam_to_bam as sb_label_spikes with:
    input:
        sam = config['data']['spikes']['sam_label']
    output:
        bam = config['data']['spikes']['bam_label']
