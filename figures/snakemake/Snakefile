import pandas as pd
import cerberus
import warnings
import os
import sys
from encoded_client.encoded import ENCODED

p = os.path.dirname(os.getcwd())
sys.path.append(p)

from scripts.utils import *
from scripts.plotting import *
from scripts.mane_utils import *

warnings.filterwarnings('ignore')

configfile: 'snakemake/config.yml'

species = ['human', 'mouse']
# species = ['human']

# dl_files = ['ab', 'filt_ab', 'read_annot']
dl_files = ['ab', 'filt_ab']
events = ['A3', 'A5', 'AF', 'AL', 'MX', 'RI', 'SE']

def get_mirna_ids(config):
    df = pd.read_csv(expand(config['data']['mirna']['files'],
                species='human')[0],
                sep='\t',
                header=None)
    df.columns = ['link']

    # remove first line
    df = df.loc[~df.link.str.contains('metadata')]

    df['id'] = df.link.str.rsplit('/', expand=True, n=1)[1]
    df['id'] = df['id'].str.split('.tsv', expand=True, n=1)[0]
    return df['id'].tolist()

# def get_lr_ids(file_format='bam', species='human'):
#     metadata = get_lr_exp_meta(species)
#     if file_format == 'bam':
#         metadata = metadata.loc[metadata.output_type=='unfiltered alignments']
#     elif file_format == 'label_bam':
#         metadata = metadata.loc[metadata.output_type=='alignments']
#     elif file_format == 'fastq':
#         metadata = metadata.loc[metadata.output_type=='reads']
#     return metadata.file.tolist()

# def get_df_lr_ids(file_id_df, file_format, species):
#     df = file_id_df.copy(deep=True)
#     if file_format == 'bam':
#         df = df.loc[df.output_type=='unfiltered alignments']
#     elif file_format == 'label_bam':
#         df = df.loc[df.output_type=='alignments']
#     elif file_format == 'fastq':
#         df = df.loc[df.output_type=='reads']
#     if species:
#         df = df.loc[df.species==species]
#     ids = df.file.tolist()
#     return ids

# lr meta
meta_df = get_meta_df(config, species)

# procap meta
procap_meta = pd.read_csv(expand(config['procap']['lib_meta'],
                                 species='human')[0],
                                 sep='\t')
procap_output_types = ['bidirectional_peaks', 'unidirectional_peaks']

# lrgasp cage meta
lrgasp_cage_meta = pd.read_csv(expand(config['lrgasp_cage']['meta'],
                                species='human')[0],
                                sep='\t')

wildcard_constraints:
    dataset='|'.join([re.escape(x) for x in meta_df.dataset.tolist()]),
    procap_dataset='|'.join([re.escape(x) for x in procap_meta.dataset.tolist()]),
    output_type='|'.join([re.escape(x) for x in procap_output_types]),

def get_col_from_meta_df(wc, col):
    temp = meta_df.copy(deep=True)
    if wc.species:
        temp = meta_df.loc[meta_df.species == wc.species]
    return temp[col].tolist()


def get_encid_from_dataset(dataset, meta, file_format):
    m = {'label_bam': 'ENCODE_alignments_id',
         'bam': 'ENCODE_unfiltered_alignments_id',
         'fastq': 'ENCODE_reads_id'}
    if file_format in list(m.keys()):
        id = meta.loc[meta.dataset == dataset, m[file_format]].values[0]
    else:
        id = meta.loc[meta.dataset == dataset, file_format].values[0]
    return id

rule all:
    input:
        # expand(config['data']['ab'], species=species),
        # expand(config['data']['talon_filt_ab'], species=species),
        # expand(config['data']['lapa_filt_ab'], species=species),
        # expand(config['data']['lapa_gtf'], species=species),
        # expand(config['data']['filt_ab'], species=species),
        # expand(config['data']['cerb_annot'], species=species),
        # expand(config['data']['cerb_gtf'], species=species),
        # expand(config['ref']['new_gencode_gtf'], species=species),
        # expand(config['ref']['cerberus']['gtf'], species=species),
        # expand(config['ref']['cerberus']['t_info'], species=species),
        # expand(config['ref']['cerberus']['g_info'], species=species), #,
        # expand(config['data']['t_info'], species=species),
        # expand(config['data']['major_isos'], species=species, obs_col='sample'),
        # expand(config['data']['exp_gene_subset'], species='human', obs_col=['sample', 'dataset']),
        # expand(config['data']['exp_gene_subset'], species='mouse', obs_col=['sample']),
        # expand(config['data']['pi_tpm']['tss'], species='human', obs_col=['sample', 'dataset']),
        # expand(config['data']['pi_tpm']['ic'], species='human', obs_col=['sample', 'dataset']),
        # expand(config['data']['pi_tpm']['tes'], species='human', obs_col=['sample', 'dataset']),
        # expand(config['data']['pi_tpm']['triplet'], species='human', obs_col=['sample', 'dataset']),
        # expand(config['data']['pi_tpm']['triplet'], species='mouse', obs_col=['sample']),
        # expand(config['ref']['t_info'], species=species),
        # expand(config['ref']['g_info'], species=species),
        # expand(config['data']['sg'], species=species),
        # expand(config['data']['mirna']['tsv'], species='human', encid=get_mirna_ids(config)),
        # expand(config['data']['bam'], species='human', encid=get_lr_ids('bam', species='human')),
        # expand(config['data']['fastq_gz'], species='human', encid=get_lr_ids('fastq', species='human')),
        # expand(config['data']['read_len_meta'], species='human'),
        # expand(config['data']['suppa']['psi'],
        #         event=events,
        #         species='human'),
        # expand(config['data']['suppa']['gtex']['events']['A3'],
        #                 species='human'),
        # expand(config['data']['suppa']['gtex']['cerb_ids'],
        #        species='human'),
        # expand(config['data']['psi'],
        #        species='human'),
        # expand(config['data']['suppa']['matching_events'],
        #        species='human',
        #        event=['AF', 'AL'])
        # expand(config['data']['p_pred']['summary'], species=species),
        # expand(config['data']['p_pred']['blastp_6'], species=species)
        config['data']['spikes']['lapa_filt_ab'],
        config['data']['spikes']['lapa_filt_gtf']
        # expand(config['data']['ca_plus'],
        #        species='human')

rule dl:
    resources:
        mem_gb = 4,
        threads = 1
    shell:
        "wget -O {output.out} {params.link}"

rule gunzip:
    resources:
        mem_gb = 4,
        threads = 1
    shell:
        "gunzip -c {input.gz} > {output.out}"

################################################################################
########################### Data download ######################################
################################################################################

use rule dl as dl_ab with:
  params:
    link = lambda w:config['encode'][w.species]['ab']
  output:
    out = config['data']['ab']

use rule dl as dl_filt_ab with:
  params:
    link = lambda w:config['encode'][w.species]['filt_ab']
  output:
    out = config['data']['filt_ab']

use rule dl as dl_read_annot with:
  params:
    link = lambda w:config['encode'][w.species]['read_annot']
  output:
    out = config['data']['read_annot']

use rule dl as dl_cerb_annot with:
  params:
    link = lambda w:config['encode'][w.species]['cerb_annot']
  output:
    out = config['data']['cerb_annot']

use rule dl as dl_ref_genome with:
  params:
    link = lambda w:config['encode'][w.species]['fa_gz']
  output:
    out = config['ref']['fa_gz']

use rule gunzip as gunzip_ref_genome with:
  input:
    gz = config['ref']['fa_gz']
  output:
    out = config['ref']['fa']

use rule dl as dl_gencode_gtf with:
  params:
    link = lambda w:config['encode'][w.species]['new_gencode_gtf_gz']
  output:
    out = config['ref']['new_gencode_gtf_gz']

use rule gunzip as gunzip_gencode with:
  input:
    gz = config['ref']['new_gencode_gtf_gz']
  output:
    out = config['ref']['new_gencode_gtf']

use rule dl as dl_gencode_fa with:
  params:
    link = lambda w:config['link'][w.species]['p_fa']
  output:
    out = config['ref']['p_fa_gz']

use rule gunzip as gunzip_gencode_fa with:
  input:
    gz = config['ref']['p_fa_gz']
  output:
    out = config['ref']['p_fa']

rule fix_gencode_fa_headers:
    input:
        fa = config['ref']['p_fa']
    resources:
        threads = 1,
        mem_gb = 4
    output:
        fa = config['ref']['p_fa_header']
    run:
        fix_fa_headers(input.fa, output.fa)

use rule dl as dl_cerb_gtf with:
  params:
    link = lambda w:config['encode'][w.species]['cerb_gtf_gz']
  output:
    out = config['data']['cerb_gtf_gz']

use rule gunzip as gunzip_cerb with:
  input:
    gz = config['data']['cerb_gtf_gz']
  output:
    out = config['data']['cerb_gtf']

use rule dl as dl_lapa_gtf with:
  params:
    link = lambda w:config['encode'][w.species]['lapa_gtf_gz']
  output:
    out = config['data']['lapa_gtf_gz']

use rule gunzip as gunzip_lapa with:
  input:
    gz = config['data']['lapa_gtf_gz']
  output:
    out = config['data']['lapa_gtf']

use rule dl as dl_talon_filt_ab with:
  params:
    link = lambda w:config['encode'][w.species]['talon_filt_ab']
  output:
    out = config['data']['talon_filt_ab']

use rule dl as dl_lapa_filt_ab with:
  params:
    link = lambda w:config['encode'][w.species]['lapa_filt_ab']
  output:
    out = config['data']['lapa_filt_ab']

rule get_chrom_sizes:
    resources:
        threads = 1,
        mem_gb = 8
    shell:
        """
        faidx {input.fa} -i chromsizes > {output.chrom_sizes}
        """

################################################################################
################################# Minimap2 #####################################
################################################################################
rule sam_to_bam:
    resources:
        threads = 16,
        mem_gb = 32
    shell:
        """
        samtools view -Sb {input.sam} > {output.bam}
        """
rule index_bam:
    resources:
        threads = 16,
        mem_gb = 32
    shell:
        """
        samtools sort {input.bam} > {output.sorted_bam}
        samtools index {output.sorted_bam}
        """

rule minimap2:
    resources:
        threads = 32,
        mem_gb = 64
    shell:
        """
        module load minimap2
        minimap2 \
            -t {resources.threads} \
            -ax splice \
            -uf \
            --secondary=no \
            -C5 \
            {input.fa} \
            {input.fastq} \
            > {output.sam}
            """

################################################################################
################################ Metadata files ################################
################################################################################
rule get_lib_meta:
    run:
        get_lib_meta_from_enc_meta(input.meta,
                                   input.biosamp_map,
                                   output.meta)

rule get_lr_file_ids:
    resources:
        mem_gb = 16,
        threads = 1
    run:
        df = pd.DataFrame()
        df['file_id'] = params.encids
        df.to_csv(output.ids, sep='\t', index=False)

# use rule get_lr_file_ids as rule get_label_bam_lr_file_ids with:
#     params:
#         encids = lambda wc:get_lr_ids('label_bam', species=wc.species)
#     output:
#         ids = config['data']['label_bam']['ids']
#
# use rule get_lr_file_ids as rule get_bam_lr_file_ids with:
#     params:
#         encids = lambda wc:get_lr_ids('bam', species=wc.species)
#     output:
#         ids = config['data']['bam']['ids']
#
# use rule get_lr_file_ids as rule get_fastq_lr_file_ids with:
#     params:
#         encids = lambda wc:get_lr_ids('fastq', species=wc.species)
#     output:
#         ids = config['data']['fastq']['ids']

################################################################################
######################### Spike download / processing ##########################
################################################################################

use rule dl as dl_sirv_gtf with:
  params:
    link = config['encode']['spikes']['sirv_gtf_gz']
  output:
    out = config['ref']['spikes']['sirv_gtf_gz']

use rule gunzip as gunzip_sirv_gtf with:
  input:
    gz = config['ref']['spikes']['sirv_gtf_gz']
  output:
    out = config['ref']['spikes']['sirv_gtf']

use rule dl as dl_sirv_fa with:
  params:
    link = config['encode']['spikes']['sirv_fa_gz']
  output:
    out = config['ref']['spikes']['sirv_fa_gz']

use rule gunzip as gunzip_sirv_fa with:
  input:
    gz = config['ref']['spikes']['sirv_fa_gz']
  output:
    out = config['ref']['spikes']['sirv_fa']

use rule dl as dl_ercc_fa with:
  params:
    link = config['encode']['spikes']['ercc_fa_gz']
  output:
    out = config['ref']['spikes']['ercc_fa_gz']

use rule gunzip as gunzip_ercc_fa with:
  input:
    gz = config['ref']['spikes']['ercc_fa_gz']
  output:
    out = config['ref']['spikes']['ercc_fa']

rule get_ercc_gtf:
    input:
        fa = config['ref']['spikes']['ercc_fa']
    resources:
        mem_gb = 16,
        threads = 1
    output:
        gtf = config['ref']['spikes']['ercc_gtf']
    shell:
        """
        python ../scripts/merge_encode_annotations.py \
            -o temp_reformat_ercc.gtf \
            {input.fa}

        python ../scripts/reformat_gtf.py \
            -o {output.gtf} \
            -gtf temp_reformat_ercc.gtf

        rm temp_reformat_ercc.gtf
        """

rule get_spike_gtf:
    input:
        sirv = config['ref']['spikes']['sirv_gtf'],
        ercc = config['ref']['spikes']['ercc_gtf']
    resources:
        mem_gb = 16,
        threads = 2
    output:
        all = config['ref']['spikes']['spike_gtf']
    shell:
        """
        cat {input.sirv} > {output.all}
        cat {input.ercc} >> {output.all}
        """

rule get_spike_fa:
    input:
        sirv = config['ref']['spikes']['sirv_fa'],
        ercc = config['ref']['spikes']['ercc_fa']
    resources:
        threads = 1,
        mem_gb = 4
    output:
        all = config['ref']['spikes']['spike_fa']
    shell:
        """
        cat {input.sirv} > {output.all}
        cat {input.ercc} >> {output.all}
        """

use rule get_chrom_sizes as get_spike_chrom_sizes with:
    input:
        fa = config['ref']['spikes']['spike_fa']
    output:
        chrom_sizes = config['ref']['spikes']['chrom_sizes']

rule get_spike_chrom_bed:
    input:
        chrom_sizes = config['ref']['spikes']['chrom_sizes']
    resources:
        mem_gb = 16,
        threads = 1
    output:
        bed = config['ref']['spikes']['chrom_bed']
    run:
        df = pd.read_csv(input.chrom_sizes,
                         sep='\t',
                         header=None,
                         names=['chr', 'stop'])
        df['start'] = 0
        df = df[['chr', 'start', 'stop']]
        df.to_csv(output.bed,
                  sep='\t',
                  header=None,
                  index=False)

################################################################################
############################# Data processing ##################################
################################################################################
def get_new_gencode_source(wildcards):
    if wildcards.species == 'human':
        source = 'v40'
    elif wildcards.species == 'mouse':
        source = 'vM25'
    return source

# annotate gencode gtf w/ cerberus
rule cerberus_update_gtf:
    input:
        annot = config['data']['cerb_annot'],
        gtf = config['ref']['new_gencode_gtf']
    output:
        update_gtf = config['ref']['cerberus']['gtf']
    resources:
        mem_gb = 56,
        threads = 1
    params:
        source = get_new_gencode_source
    shell:
        "cerberus replace_gtf_ids \
            --h5 {input.annot} \
            --gtf {input.gtf} \
            --source {params.source} \
            --update_ends \
            --collapse \
            -o {output.update_gtf}"

# get transcript info
rule get_t_info:
    resources:
        mem_gb = 56,
        threads = 1
    run:
        get_transcript_info(input.gtf, output.o)

use rule get_t_info as ref_cerb_t_info with:
    input:
        gtf = config['ref']['cerberus']['gtf']
    output:
        o = config['ref']['cerberus']['t_info']

use rule get_t_info as ref_t_info with:
    input:
        gtf = config['ref']['new_gencode_gtf']
    output:
        o = config['ref']['t_info']

use rule get_t_info as cerb_t_info with:
    input:
        gtf = config['data']['cerb_gtf']
    output:
        o = config['data']['t_info']

# get gene info
rule get_g_info:
    resources:
        mem_gb = 56,
        threads = 1
    run:
        get_gene_info(input.gtf, output.o)

use rule get_g_info as cerb_ref_g_info with:
    input:
        gtf = config['ref']['cerberus']['gtf']
    output:
        o = config['ref']['cerberus']['g_info']

use rule get_g_info as ref_g_info with:
    input:
        gtf = config['ref']['new_gencode_gtf']
    output:
        o = config['ref']['g_info']

################################################################################
################################## Swan ########################################
################################################################################
def make_sg(input, params, wildcards):

    # initialize
    sg = swan.SwanGraph()
    sg.add_annotation(input.annot)
    sg.add_transcriptome(input.gtf, include_isms=True)
    sg.add_abundance(input.ab)
    sg.add_abundance(input.gene_ab, how='gene')
    sg.save_graph(params.prefix)

    # add metadata and add colors
    sg.add_metadata(input.meta)
    c_dict, order = get_biosample_colors(wildcards.species)
    sg.set_metadata_colors('sample', c_dict)

    # human only settings
    if wildcards.species == 'human':
        c_dict, order = get_ad_colors()
        sg.set_metadata_colors('health_status', c_dict)
    # save
    sg.save_graph(params.prefix)

rule swan_init:
    input:
        annot = config['ref']['cerberus']['gtf'],
        ab = config['data']['filt_ab'],
        gene_ab = config['data']['ab'],
        gtf = config['data']['cerb_gtf'],
        meta = config['data']['meta']
    params:
        prefix = config['data']['sg'].replace('.p', '')
    resources:
        mem_gb = 64,
        threads = 1
    output:
        sg = config['data']['sg']
    run:
        make_sg(input, params, wildcards)

rule major_isos:
    input:
        sg = config['data']['sg'],
        filt_ab = config['data']['filt_ab']
    resources:
        mem_gb = 16,
        threads = 8
    params:
        min_tpm = 1,
        gene_subset = 'polya'
    output:
        ofile = config['data']['major_isos']
    run:
        get_major_isos(input.sg,
                       input.filt_ab,
                       wildcards.obs_col,
                       output.ofile,
                       min_tpm=params.min_tpm,
                       gene_subset=params.gene_subset)

################################################################################
################################## MANE ########################################
################################################################################
rule get_exp_genes:
    input:
        ab = config['data']['ab']
    resources:
        mem_gb = 16,
        threads = 8
    params:
        min_tpm = 1,
        gene_subset = 'polya',
    output:
        ofile = config['data']['exp_gene_subset']
    run:
        get_exp_gene_subset(input.ab,
                            params.min_tpm,
                            wildcards.obs_col,
                            output.ofile,
                            wildcards.species)

rule get_pi_tpm:
    input:
        swan_file = config['data']['sg']
    resources:
        mem_gb = 32,
        threads = 8
    params:
        odir = config['data']['pi_tpm']['tss'].rsplit('/', maxsplit=1)[0]
    output:
        config['data']['pi_tpm']['tss'],
        config['data']['pi_tpm']['ic'],
        config['data']['pi_tpm']['tes'],
        config['data']['pi_tpm']['triplet']
    run:
        sg = swan.read(input.swan_file)
        get_pi_tpm_tables(sg, wildcards.obs_col, params.odir)

################################################################################
################################## miRNA #######################################
################################################################################

rule dl_mirna:
    resources:
        mem_gb = 32,
        threads = 8
    output:
        tsv = config['data']['mirna']['tsv']
    shell:
        "wget https://www.encodeproject.org/files/{wildcards.encid}/@@download/{wildcards.encid}.tsv -O {output.tsv}"

################################################################################
################################## Procap ######################################
################################################################################
use rule get_lib_meta as procap_lib_meta with:
    input:
        meta = config['procap']['meta'],
        biosamp_map = config['ref']['biosamp_map']
    output:
        meta = config['procap']['lib_meta']

rule dl_procap:
    resources:
        mem_gb = 32,
        threads = 1
    params:
        encid = lambda w:get_encid_from_dataset(w.procap_dataset,
                                                procap_meta,
                                                w.output_type)
    output:
        bed = config['procap']['bed_gz']
    shell:
        "wget https://www.encodeproject.org/files/{params.encid}/@@download/{params.encid}.bed.gz -O {output.bed}"

use rule gunzip as gz_procap with:
    input:
        gz = config['procap']['bed_gz']
    output:
        out = config['procap']['bed']

rule reformat_procap_uni:
    input:
        bed = config['procap']['bed']
    resources:
        mem_gb = 10,
        threads = 1
    output:
        bed = config['procap']['format_uni_bed']
    run:
        df = pd.read_csv(input.bed, sep='\t',
                         header=None,
                         names=['Chromosome', 'Start',
                                'End', 'idk1', 'idk2',
                                'Strand'], usecols=[i for i in range(6)])
        df.drop(['idk1', 'idk2'], axis=1, inplace=True)
        pr.PyRanges(df).to_bed(output.bed)

################################################################################
############################## LRGASP CAGE #####################################
################################################################################
def get_col_from_dataset(dataset, meta, col):
    return meta.loc[meta.dataset==dataset, col].values[0]

use rule dl as dl_lrgasp_cage with:
    params:
        link = lambda w:get_col_from_dataset(w.lrgasp_cage_dataset,
                                             lrgasp_cage_meta,
                                             'link')
    output:
        out = config['lrgasp_cage']['bed_gz']

use rule gunzip as gz_lrgasp_cage with:
    input:
        gz = config['lrgasp_cage']['bed_gz']
    output:
        out = config['lrgasp_cage']['bed']

################################################################################
########################### Adding adtl support ################################
################################################################################

def get_tss_ca_add_settings(wc, how):
    """
    Get files or sources for additional tss support

    Parameters:
        how (str): {'file', 'source'}
    """

    files = []
    sources = []

    # procap bidirectional
    files += expand(config['procap']['bed'],
                    procap_dataset=procap_meta.dataset.unique().tolist(),
                    species=wc.species,
                    output_type='bidirectional_peaks')
    sources += [d+'_bi_procap' for d in procap_meta.dataset.unique().tolist()]

    # procap unidirectional
    files += expand(config['procap']['format_uni_bed'],
                    procap_dataset=procap_meta.dataset.unique().tolist(),
                    species=wc.species,
                    output_type='unidirectional_peaks')
    sources += [d+'_uni_procap' for d in procap_meta.dataset.unique().tolist()]

    # lrgasp cage
    files += expand(config['lrgasp_cage']['bed'],
                    lrgasp_cage_dataset=lrgasp_cage_meta.dataset.unique().tolist(),
                    species=wc.species)
    sources += [d+'_lrgasp_cage' for d in lrgasp_cage_meta.dataset.unique().tolist()]

    if how == 'file':
        return files
    elif how == 'source':
        return sources

rule add_tss_to_ca:
    input:
        files = lambda w:get_tss_ca_add_settings(w,'file'),
        h5 = config['data']['cerb_annot']
    params:
        sources = lambda w:get_tss_ca_add_settings(w,'source'),
        slack = 20,
        ref = False,
        add_ends = False
    output:
        h5 = config['data']['ca_plus']
    run:
        ca = cerberus.read(input.h5)
        refs = False
        add_ends = False
        for bed, source in zip(input.files, params.sources):
            ca.add_bed(bed, False, False, source, 'tss', slack=20)
        ca.write(output.h5)

################################################################################
############################## Diane's stuff ###################################
################################################################################
rule dl_lr_bam:
    resources:
        mem_gb = 32,
        threads = 8
    params:
        encid = lambda w:get_encid_from_dataset(w.dataset,
                                                meta_df,
                                                'bam')
    output:
        bam = temporary(config['data']['bam'])
    shell:
        "wget https://www.encodeproject.org/files/{params.encid}/@@download/{params.encid}.bam -O {output.bam}"

rule dl_lr_label_bam:
    resources:
        mem_gb = 32,
        threads = 8
    params:
        encid = lambda w:get_encid_from_dataset(w.dataset,
                                                meta_df,
                                                'label_bam')
    output:
        bam = temporary(config['data']['label_bam'])
    shell:
        "wget https://www.encodeproject.org/files/{params.encid}/@@download/{params.encid}.bam -O {output.bam}"


rule dl_lr_fastq:
    resources:
        mem_gb = 32,
        threads = 8
    params:
        encid = lambda w:get_encid_from_dataset(w.dataset,
                                                meta_df,
                                                'fastq')
    output:
        fastq = temporary(config['data']['fastq_gz'])
    shell:
        "wget https://www.encodeproject.org/files/{params.encid}/@@download/{params.encid}.fastq.gz -O {output.fastq}"

rule get_lr_read_lens:
    input:
        bams = expand(config['data']['bam'],
                      species='human',
                      dataset=lambda w:get_col_from_meta_df(wc, col='dataset')),
        fastqs = expand(config['data']['fastq_gz'],
                        species='human',
                        dataset=lambda w:get_col_from_meta_df(wc, col='dataset'))
    resources:
        mem_gb = 32,
        threads = 8
    output:
        tsv = config['data']['read_len_meta']
    run:
        get_lr_read_lens(input.bams, input.fastqs, output.tsv)

################################################################################
################################# SUPPA ########################################
################################################################################

use rule dl as dl_gtex_gtf with:
  params:
    link = lambda w:config['encode'][w.species]['gtex_gtf_gz']
  output:
    out = config['ref']['gtex_gtf_gz']

use rule gunzip as gunzip_gtex with:
  input:
    gz = config['ref']['gtex_gtf_gz']
  output:
    out = config['ref']['gtex_gtf']

rule get_transcript_novelties:
    input:
        annot = config['data']['cerb_annot'],
        filt_ab = config['data']['filt_ab'],
        t_meta = config['data']['t_info']
    params:
        min_tpm = 1,
        gene_subset = 'polya',
        ver = 'v40_cereberus'
    resources:
        mem_gb = 8,
        threads = 1
    output:
        novelties = config['data']['novelties']
    run:
        get_transcript_novelties(input.annot,
                                 input.filt_ab,
                                 input.t_meta,
                                 params.min_tpm,
                                 params.gene_subset,
                                 params.ver,
                                 output.novelties)

rule preproc_suppa:
    input:
        gtf = config['data']['cerb_gtf'],
        nov = config['data']['novelties'],
        filt_ab = config['data']['filt_ab']
    params:
        temp_filt_ab_nov = 'temp_filt_ab_nov.tsv',
        temp_filt_ab_nov_2 = 'temp_filt_ab_nov_2.tsv'
    output:
        filt_ab = config['data']['suppa']['filt_ab'],
        gtf = config['data']['suppa']['gtf']
    conda:
        'seurat'
    shell:
        """
        sed 's/,/_/g' {input.gtf} > {output.gtf}
        Rscript ../scripts/filter_abundance_mat_by_novelty.R \
            --novelties {input.nov} \
            --filtab {input.filt_ab} \
            --ofile {params.temp_filt_ab_nov}
        cut -f4,12- {params.temp_filt_ab_nov} | sed 's/,/_/g' > {params.temp_filt_ab_nov_2}
        python ../scripts/suppa_format_rm_colname.py {params.temp_filt_ab_nov_2}
        tail -c +2 {params.temp_filt_ab_nov_2} > {output.filt_ab}
        """

rule suppa_generate_events:
    resources:
        mem_gb = 16,
        threads = 1
    shell:
        """
        suppa.py generateEvents \
            -i {input.gtf} \
            -o {params.opref} \
            -f ioe \
            -e SE SS MX RI FL
        """

use rule suppa_generate_events as ge_cerb with:
    input:
        gtf = config['data']['suppa']['gtf']
    params:
        opref = config['data']['suppa']['events']['A3'].rsplit('_', maxsplit=2)[0]
        # opref = config['data']['suppa']['events']
    output:
        config['data']['suppa']['events']['A3'],
        config['data']['suppa']['events']['A5'],
        config['data']['suppa']['events']['AF'],
        config['data']['suppa']['events']['AL'],
        config['data']['suppa']['events']['MX'],
        config['data']['suppa']['events']['RI'],
        config['data']['suppa']['events']['SE'],

use rule suppa_generate_events as ge_gtex with:
    input:
        gtf = config['ref']['gtex_gtf']
    params:
        opref = config['data']['suppa']['gtex']['events']['A3'].rsplit('_', maxsplit=2)[0]
    output:
        config['data']['suppa']['gtex']['events']['A3'],
        config['data']['suppa']['gtex']['events']['A5'],
        config['data']['suppa']['gtex']['events']['AF'],
        config['data']['suppa']['gtex']['events']['AL'],
        config['data']['suppa']['gtex']['events']['MX'],
        config['data']['suppa']['gtex']['events']['RI'],
        config['data']['suppa']['gtex']['events']['SE']

rule suppa_psi:
    resources:
        mem_gb = 16,
        threads = 1
    shell:
        """
        suppa.py psiPerEvent \
            --ioe-file {input.ioe} \
            --expression-file {input.filt_ab} \
            --o {params.opref}
        """

use rule suppa_psi as psi_cerb with:
    input:
        ioe = lambda w:config['data']['suppa']['events'][w.event],
        filt_ab = config['data']['suppa']['filt_ab']
    params:
        opref = config['data']['suppa']['psi'].rsplit('.psi', maxsplit=1)[0]
    output:
        out = config['data']['suppa']['psi']

rule get_gtex_cerb_ids:
    input:
        cerb_annot = config['data']['cerb_annot']
    resources:
        mem_gb = 2,
        threads = 1
    output:
        cerb_ids = config['data']['suppa']['gtex']['cerb_ids']
    run:
        get_gtex_cerberus_ids(input.cerb_annot, output.cerb_ids)


rule get_cerberus_psi:
    input:
        filt_ab = config['data']['filt_ab']
    params:
        min_tpm = 1,
        gene_subset = 'polya'
    resources:
        threads = 1,
        mem_gb = 64
    output:
        ofile = config['data']['psi']
    run:
        get_cerberus_psi(input.filt_ab,
                         params.min_tpm,
                         params.gene_subset,
                         output.ofile)

e_map = {'tss': 'AF',
      'tes': 'AL',
      'AL': 'tes',
      'AF': 'tss'}
rule get_matching_events:
    input:
        cerb_file = config['data']['psi'],
        suppa_file = config['data']['suppa']['psi'],
        lib_meta = config['data']['meta']
    params:
        kind = lambda w:e_map[w.event]
    output:
        ofile = config['data']['suppa']['matching_events']
    run:
        get_cerb_suppa_matching_events(input.cerb_file,
                                       input.suppa_file,
                                       output.ofile,
                                       input.lib_meta,
                                       params.kind)



# use rule get_matching_events as gme_al with:
#
#
# use rule get_matching_events as gme_al with:
#     input:
#         cerb_file = config['data']['psi'],
#         suppa_file = config['data']['suppa']['psi'],
#         lib_meta = config['data']['meta'],
#     output:
#         ofile = config['data']['suppa']['matching_events']

################################################################################
######################### Milad's TSS prediction ###############################
################################################################################

# TODO

# automate downloading the bigwigs, getting the processed tss data from cerberus
# and lapa, and downloading the jamboree tss files

################################################################################
##################### Narges's short-read data proc ############################
################################################################################

# TODO

################################################################################
######################## Sam's gene expression #################################
################################################################################

# TODO

################################################################################
################################ Protein prediction ############################
################################################################################

rule tama_gtf_to_bed:
    input:
        gtf = config['data']['cerb_gtf']
    params:
        tama_dir = config['tama_dir']
    resources:
        threads = 4,
        mem_gb = 32
    output:
        bed = config['data']['p_pred']['gtf_bed']
    shell:
        """
        python {params.tama_dir}/format_converter/tama_format_gtf_to_bed12_ensembl.py \
          {input.gtf} \
          {output.bed}
        """

rule bed_to_fasta:
    input:
        bed = config['data']['p_pred']['gtf_bed'],
        ref = config['ref']['fa']
    output:
        fa = config['data']['p_pred']['fa']
    resources:
        threads = 4,
        mem_gb = 32
    shell:
        """
        # module load bedtools2
        bedtools getfasta \
            -name \
            -split \
            -s \
            -fi {input.ref} \
            -bed {input.bed} \
            -fo {output.fa}
        """

rule tama_orf:
    input:
        fa = config['data']['p_pred']['fa']
    params:
        tama_dir = config['tama_dir']
    resources:
        threads = 4,
        mem_gb = 32
    output:
        orf_fa = config['data']['p_pred']['orf_fa']
    shell:
        """
        python {params.tama_dir}/orf_nmd_predictions/tama_orf_seeker.py \
            -f {input.fa} \
            -o {output.orf_fa}
        """

rule make_blastp_db:
    input:
        fa = config['ref']['p_fa_header']
    resources:
        mem_gb = 64,
        threads = 32
    params:
        opref = config['ref']['p_db'].replace('.pdb', '')

    output:
        db = config['ref']['p_db']
    shell:
        """
        makeblastdb \
            -in {input.fa} \
             -dbtype prot \
             -parse_seqids \
             -out {params.opref}
        """

# rule blast:
#     input:
#         orf_fa = config['data']['p_pred']['orf_fa'],
#         db = config['ref']['p_db']
#     params:
#         opref = config['ref']['p_db'].replace('.pdb', '')
#     resources:
#         mem_gb = 32,
#         threads = 32
#     output:
#         out = config['data']['p_pred']['blastp']
#     shell:
#         """
#          blastp \
#           -evalue 1e-10 \
#           -num_threads 32 \
#           -db {params.opref} \
#           -outfmt 0 \
#           -query {input.orf_fa} > \
#           {output.out}
#         """

rule blast_6:
    input:
        orf_fa = config['data']['p_pred']['orf_fa'],
        db = config['ref']['p_db']
    params:
        opref = config['ref']['p_db'].replace('.pdb', '')
    resources:
        mem_gb = 32,
        threads = 32
    output:
        out = config['data']['p_pred']['blastp_6']
    shell:
        """
         blastp \
          -evalue 1e-10 \
          -num_threads 32 \
          -db {params.opref} \
          -outfmt "6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore slen qlen" \
          -query {input.orf_fa} > \
          {output.out}
        """

# rule parse_blastp:
#     input:
#         blastp = config['data']['p_pred']['blastp_6']
#     params:
#         tama_dir = config['tama_dir']
#     resources:
#         mem_gb = 32,
#         threads = 1
#     output:
#         parsed = config['data']['p_pred']['blastp_parsed']
#     shell:
#         """
#         python {params.tama_dir}/orf_nmd_predictions/tama_orf_blastp_parser.py \
#           -b {input.blastp} \
#           -o {output.parsed}
#         """

rule parse_blastp:
    input:
        blastp = config['data']['p_pred']['blastp_6'],
        fa = config['data']['p_pred']['orf_fa']
    resources:
        mem_gb = 32,
        threads = 1
    output:
        parsed = config['data']['p_pred']['blastp_parsed']
    run:
        parse_blastp(input.blastp, input.fa, output.parsed)


rule tama_cds_bed_regions:
    input:
        parsed = config['data']['p_pred']['blastp_parsed'],
        gtf_bed = config['data']['p_pred']['gtf_bed'],
        fa = config['data']['p_pred']['fa']
    params:
        tama_dir = config['tama_dir']
    resources:
        mem_gb = 4,
        threads = 1
    output:
        bed = config['data']['p_pred']['cds_bed']
    shell:
        """
        python {params.tama_dir}/orf_nmd_predictions/tama_cds_regions_bed_add.py \
            -p {input.parsed} \
            -a {input.gtf_bed} \
            -f {input.fa} \
            -o {output.bed}
        """

rule get_pp_summary:
    input:
        fa = config['data']['p_pred']['orf_fa'],
        parsed = config['data']['p_pred']['blastp_parsed'],
        cds = config['data']['p_pred']['cds_bed']
    resources:
        mem_gb = 4,
        threads = 1
    output:
        summary = config['data']['p_pred']['summary']
    run:
        get_pp_info(input.fa,
                    input.cds,
                    input.parsed,
                    output.summary)

################################################################################
############################# TranscriptClean ##################################
################################################################################
rule tc_sjs:
    resources:
        mem_gb = 16,
        threads = 4
    params:
        tc = config['tc_path']
    shell:
        """
        python {params.tc}accessory_scripts/get_SJs_from_gtf.py \
             --f {input.gtf} \
             --g {input.fa} \
             --minIntronSize 21 \
             --o {output.sjs}
        """
rule tc:
    resources:
        mem_gb = 80,
        threads = 16
    shell:
        """
        python {params.tc}TranscriptClean.py \
            --sam {input.align} \
            --genome {input.fa} \
            --spliceJns {input.sjs} \
            --maxLenIndel 5 \
            --maxSJOffset 5 \
            -m true \
            -i true \
            --deleteTmp \
            --tmpDir {params.opref}_temp/ \
            --correctSJs true \
            --primaryOnly \
            --outprefix {params.opref} \
            --threads {resources.threads} \
            --canonOnly
        """

################################################################################
############################# TALON utilities ##################################
################################################################################
rule talon_label:
    resources:
        mem_gb = 16,
        threads = 1
    shell:
        """
        talon_label_reads \
            --f {input.align} \
            --g {input.fa} \
            --tmpDir {params.opref} \
            --ar 20  \
            --t {resources.threads} \
            --deleteTmp  \
            --o {params.opref}
        """

def get_spike_talon_run_files():
    temp = meta_df.loc[meta_df.spikeins==True].copy(deep=True)
    files = expand(config['data']['spikes']['bam_label'],
                   zip,
                   species=temp.species.tolist(),
                   dataset=temp.dataset.tolist())
    return files

rule talon_config:
    input:
        files = get_spike_talon_run_files()
    resources:
        threads = 1,
        mem_gb = 1
    output:
        config_file = config['data']['spikes']['talon_config']
    run:
        df = pd.DataFrame()
        df['file'] = list(input.files)
        df['file_base'] = df.file.str.rsplit('/', n=1, expand=True)[1]
        df['dataset'] = df.file_base.str.rsplit('.bam', n=1, expand=True)[0]

        # get species from path and append to sample / dataset
        df['species'] = np.nan
        df.loc[df.file.str.contains('human'), 'species'] = 'human'
        df.loc[df.file.str.contains('mouse'), 'species'] = 'mouse'
        df['dataset'] = df['dataset']+'_'+df['species']
        df['sample'] = df['dataset']

        # all platforms are pacbio
        df['platform'] = 'PacBio'

        # write
        df = df[['dataset', 'sample', 'platform', 'file']]
        df.to_csv(output.config_file, header=None, sep=',', index=False)

rule talon_init:
	input:
		ref_gtf = config['ref']['spikes']['spike_gtf']
	output:
		db = config['ref']['spikes']['talon_db']
	params:
		talon_opref = config['ref']['spikes']['talon_db'].rsplit('.db', maxsplit=1)[0],
		genome = 'spikes',
		annot = 'spikes',
		idprefix = 'spikes'
	resources:
		mem_gb = 32,
		threads = 16
	shell:
		'talon_initialize_database \
    		--f {input.ref_gtf} \
    		--g {params.genome} \
    		--a {params.annot} \
    		--l 0 \
    		--idprefix {params.idprefix} \
    		--5p 500 \
    		--3p 300 \
    		--o {params.talon_opref}'

rule talon:
    input:
        ref = config['ref']['spikes']['talon_db'],
        config = config['data']['spikes']['talon_config']
    params:
        genome = 'spikes',
        opref = config['data']['spikes']['talon_db'].rsplit('_talon', maxsplit=1)[0],
    resources:
        mem_gb = 256,
        threads = 30
    output:
        db = config['data']['spikes']['talon_db'],
        annot = config['data']['spikes']['read_annot']
    shell:
        """
        ref_db={input.ref}_back
        cp {input.ref} ${{ref_db}}
        talon \
            --f {input.config} \
            --db ${{ref_db}} \
            --build {params.genome} \
            --tmpDir {params.opref}_temp/ \
            --threads {resources.threads} \
            --o {params.opref}
        mv ${{ref_db}} {params.opref}_talon.db
        """

rule talon_unfilt_ab:
    input:
        db = config['data']['spikes']['talon_db']
    resources:
        threads = 1,
        mem_gb = 32
    params:
        genome = 'spikes',
        annot = 'spikes',
        opref = config['data']['spikes']['ab'].rsplit('_talon', maxsplit=1)[0]
    output:
        ab = config['data']['spikes']['ab']
    shell:
        """
        talon_abundance \
            --db {input.db} \
            -a {params.annot} \
            -b {params.genome} \
            --o {params.opref}
        """

rule talon_filt:
    input:
        db = config['data']['spikes']['talon_db']
    resources:
        threads = 1,
        mem_gb = 128
    params:
        annot = 'spikes'
    output:
        list = config['data']['spikes']['filt_list']
    shell:
        """
        talon_filter_transcripts \
            --db {input.db} \
            -a {params.annot} \
            --maxFracA=0.5 \
            --minCount=5 \
            --minDatasets=2 \
            --o {output.list}
        """

rule talon_filt_ab:
    input:
        db = config['data']['spikes']['talon_db'],
        filt = config['data']['spikes']['filt_list']
    resources:
        threads = 1,
        mem_gb = 128
    params:
        genome = 'spikes',
        annot = 'spikes',
        opref = config['data']['spikes']['filt_ab'].rsplit('_talon', maxsplit=1)[0]
    output:
        filt_ab = config['data']['spikes']['filt_ab']
    shell:
        """
        talon_abundance \
            --db {input.db} \
            -a {params.annot} \
            -b {params.genome} \
            --whitelist {input.filt} \
            --o {params.opref}
        """

rule talon_gtf:
    input:
        db = config['data']['spikes']['talon_db'],
        filt = config['data']['spikes']['filt_list']
    resources:
        threads = 1,
        mem_gb = 128
    params:
        genome = 'spikes',
        annot = 'spikes',
        opref = config['data']['spikes']['filt_gtf'].rsplit('_talon', maxsplit=1)[0]
    output:
        gtf = config['data']['spikes']['filt_gtf']
    shell:
        """
        talon_create_GTF \
            --db {input.db} \
            -b {params.genome} \
            -a {params.annot} \
            --whitelist {input.filt} \
            --observed \
            --o {params.opref}
        """

rule talon_gene_counts:
    input:
        ab = config['data']['spikes']['ab']
    output:
        gene_ab = config['data']['spikes']['gene_ab']
    resources:
        mem_gb = 64,
        threads = 1
    run:
        get_gene_counts_matrix(input.ab, output.gene_ab)

################################################################################
################################# LAPA #########################################
################################################################################
def get_lapa_settings(wc, lapa_ends, kind):
    lapa_ends = expand(lapa_ends,
                       zip,
                       end_mode=wc.end_mode)[0]
    if kind == 'temp_file':
        temp = os.path.dirname(lapa_ends)+'/'
        if wc.end_mode == 'tes':
            temp += 'polyA_clusters.bed'
        elif wc.end_mode == 'tss':
            temp += 'tss_clusters.bed'
        return temp
    elif kind == 'lapa_cmd':
        if wc.end_mode == 'tes':
            return 'lapa'
        elif wc.end_mode == 'tss':
            return 'lapa_tss'

# def get_study_datasets(wc, dataset_df):
#     datasets = dataset_df.loc[dataset_df.study==wc.study, 'dataset'].tolist()
#     return datasets

def get_lapa_run_info(wc, df, config_entry=None, col=False):
    temp = df.loc[df.spikeins==True].copy(deep=True)
    datasets = temp.dataset.tolist()
    species = temp.species.tolist()
    samples = temp['sample'].tolist()
    temp = pd.DataFrame()
    if config_entry:
        files = expand(config_entry,
                       zip,
                       species=species,
                       dataset=datasets)
        temp['path'] = files
    temp['dataset'] = datasets
    temp['sample'] = samples
    # temp = temp[['sample', 'dataset']]
    if col:
        return temp[col].tolist()
    else:
        return temp

rule lapa_config:
    resources:
        threads = 1,
        mem_gb = 1
    run:
        config = get_lapa_run_info(wildcards, params.df) # todo
        config = config[['sample', 'dataset']].copy(deep=True)
        config['path'] = input.files
        config.columns = ['sample', 'dataset', 'path']
        config.to_csv(output.config, sep=',', index=False)

rule lapa_call_ends:
    resources:
        threads = 4,
        mem_gb = 32
    shell:
        """
        rm -rf {params.opref}
        {params.lapa_cmd} \
            --alignment {input.config} \
            --fasta {input.fa} \
            --annotation {input.gtf} \
            --chrom_sizes {input.chrom_sizes} \
            --output_dir {params.opref}
        if [ {params.lapa_end_temp} != {output.ends} ]
        then
            cp {params.lapa_end_temp} {output.ends}
        fi
        """

rule lapa_link:
    resources:
        threads = 1,
        mem_gb = 256
    shell:
        """
        lapa_link_tss_to_tes \
            --alignment {input.annot} \
            --lapa_dir {params.tes_dir} \
            --lapa_tss_dir {params.tss_dir} \
            --output {output.links}
        """

rule lapa_correct_talon:
    resources:
        threads = 1,
        mem_gb = 64
    shell:
        """
        lapa_correct_talon \
                --links {input.links} \
                --read_annot {input.annot} \
                --gtf_input {input.gtf} \
                --gtf_output {output.gtf} \
                --abundance_input {input.ab} \
                --abundance_output {output.ab} \
                --keep_unsupported
        """

# update lapa gtf w/ novelty category "ISM_rescue"
rule update_lapa_gtf:
    resources:
        mem_gb = 64,
        threads = 1
    run:
        gtf = pr.read_gtf(input.lapa_gtf, as_df=True)
        gtf.loc[(gtf.transcript_id.str.contains('#'))&(gtf.ISM_transcript=='TRUE'),
                 'transcript_novelty'] = 'ISM_rescue'
        gtf = pr.PyRanges(gtf)
        gtf.to_gtf(output.nov_gtf)

# update lapa abundance file w/ novelty category "ISM_rescue"
rule update_lapa_ab:
    resources:
        mem_gb = 64,
        threads = 1
    run:
        df = pd.read_csv(input.lapa_ab, sep='\t')
        df.loc[(df.annot_transcript_id.str.contains('#'))&(df.transcript_novelty=='ISM'), 'transcript_novelty'] = 'ISM_rescue'
        df.to_csv(output.nov_ab, sep='\t', index=False)

def filter_lapa(ab, novs):
    df = pd.read_csv(ab, sep='\t')
    # novs = ['Known', 'NIC', 'NNC', 'ISM_rescue']
    df = df.loc[df.transcript_novelty.isin(novs)]
    df = df.loc[df.gene_novelty == 'Known']
    tids = df.annot_transcript_id.tolist()
    return tids

def filter_no_t_genes(df):
    gids = df.loc[df.Feature == 'transcript', 'gene_id'].unique().tolist()
    df = df.loc[df.gene_id.isin(gids)]
    return df

def filter_spikes(df):
    df = df.loc[~df.Chromosome.str.contains('SIRV')]
    df = df.loc[~df.Chromosome.str.contains('ERCC')]

    return df

def filter_novel_genes(df):
    df = df.loc[df.gene_novelty=='Known']
    return df

# create abundance file only with novelty categories we want
rule filter_lapa_ab:
    params:
        novs = ['Known', 'NIC', 'NNC', 'ISM_rescue']
    resources:
        mem_gb = 64,
        threads = 1
    run:
        tids = filter_lapa(input.nov_ab, params.novs)
        df = pd.read_csv(input.nov_ab, sep='\t')
        df = df.loc[df.annot_transcript_id.isin(tids)]
        df.to_csv(output.filt_ab, sep='\t', index=False)

# create gtf only with novelty categories we want
# and remove sirv / ercc
rule filter_lapa_gtf:
    params:
        filter_spikes = True,
        novs = ['Known', 'NIC', 'NNC', 'ISM_rescue']
    resources:
        mem_gb = 64,
        threads = 1
    run:
        # remove novelty categories we don't want
        tids = filter_lapa(input.nov_ab, params.novs)
        gtf = pr.read_gtf(input.nov_gtf).df
        gtf = gtf.loc[(gtf.transcript_id.isin(tids))|(gtf.Feature=='gene')]

        # remove sirvs / erccs
        if params.filter_spikes:
            gtf = filter_spikes(gtf)

        # remove genes with no transcripts
        gtf = filter_no_t_genes(gtf)

        gtf = pr.PyRanges(gtf)
        gtf.to_gtf(output.filt_gtf)

################################################################################
############################## Spike data proc #################################
################################################################################
use rule minimap2 as map_spikes with:
    input:
        fastq = config['data']['fastq_gz'],
        fa = config['ref']['spikes']['spike_fa']
    output:
        # sam = temporary(config['data']['minimap']['sam'])
        sam = config['data']['minimap']['sam']

use rule sam_to_bam as sb_spikes with:
    input:
        sam = config['data']['minimap']['sam']
    output:
        bam = config['data']['minimap']['bam']

rule get_spike_reads:
    input:
        bam = config['data']['minimap']['bam'],
        spike_bed = config['ref']['spikes']['chrom_bed']
    conda:
        'samtools'
    resources:
        mem_gb = 32,
        threads = 8
    output:
        sorted_bam = temporary(config['data']['minimap']['bam_sorted']),
        bam = config['data']['spikes']['bam'],
        sam = temporary(config['data']['spikes']['sam'])
    shell:
        """
        samtools sort {input.bam} > {output.sorted_bam}
        samtools index {output.sorted_bam}
        samtools view -bh --regions-file {input.spike_bed} {output.sorted_bam} > {output.bam}
        samtools view -h {output.bam} > {output.sam}
        """

use rule tc_sjs as tc_sjs_spike with:
    input:
        gtf = config['ref']['spikes']['spike_gtf'],
        fa = config['ref']['spikes']['spike_fa']
    output:
        sjs = config['ref']['spikes']['sjs']

use rule tc as tc_spike with:
    input:
        align = config['data']['spikes']['sam'],
        fa = config['ref']['spikes']['spike_fa'],
        sjs = config['ref']['spikes']['sjs']
    params:
        tc = config['tc_path'],
        opref = config['data']['spikes']['sam_clean'].split('_clean.sam', maxsplit=1)[0]
    output:
        # sam = temporary(config['data']['spikes']['sam_clean'])
        sam = config['data']['spikes']['sam_clean']


use rule talon_label as talon_label_spike with:
        input:
            align = config['data']['spikes']['sam_clean'],
            fa = config['ref']['spikes']['spike_fa'],
        params:
            a_range = 20,
            opref = config['data']['spikes']['sam_label'].rsplit('_labeled.sam', maxsplit=1)[0]
        output:
            bam = config['data']['spikes']['sam_label']

use rule sam_to_bam as sb_label_spikes with:
    input:
        sam = config['data']['spikes']['sam_label']
    output:
        bam = config['data']['spikes']['bam_label']

use rule index_bam as spike_index_bam with:
    input:
        bam = config['data']['spikes']['bam_label']
    output:
        sorted_bam = temporary(config['data']['spikes']['bam_label_sorted']),
        bam_ind = config['data']['spikes']['bam_label_index']

use rule lapa_config as spike_lapa_config with:
    input:
        files = lambda wc: get_lapa_run_info(wc, meta_df, config['data']['spikes']['bam_label_sorted'], 'path'),
        inds = lambda wc: get_lapa_run_info(wc, meta_df, config['data']['spikes']['bam_label_index'], 'path'),
    params:
        df = meta_df
    output:
        config = config['data']['spikes']['lapa_config']

use rule lapa_call_ends as spike_lapa_call_ends with:
    input:
        config = config['data']['spikes']['lapa_config'],
        fa = config['ref']['spikes']['spike_fa'],
        gtf = config['ref']['spikes']['spike_gtf'],
        chrom_sizes = config['ref']['spikes']['chrom_sizes'],
        files = lambda wc: get_lapa_run_info(wc, meta_df, config['data']['spikes']['bam_label_sorted'], 'path'),
        inds = lambda wc: get_lapa_run_info(wc, meta_df, config['data']['spikes']['bam_label_index'], 'path')
    params:
        opref = config['data']['spikes']['lapa_ends'].rsplit('/', maxsplit=1)[0]+'/',
        lapa_cmd = lambda wc: get_lapa_settings(wc, config['data']['spikes']['lapa_ends'], 'lapa_cmd'),
        lapa_end_temp = lambda wc: get_lapa_settings(wc, config['data']['spikes']['lapa_ends'], 'temp_file')
    output:
        ends = config['data']['spikes']['lapa_ends']

use rule lapa_link as spike_lapa_link with:
    input:
        annot = config['data']['spikes']['read_annot'],
        tss = expand(config['data']['spikes']['lapa_ends'], end_mode='tss', allow_missing=True)[0],
        tes = expand(config['data']['spikes']['lapa_ends'], end_mode='tes', allow_missing=True)[0]
    params:
        tss_dir = expand(config['data']['spikes']['lapa_ends'], end_mode='tss', allow_missing=True)[0].rsplit('/', maxsplit=1)[0]+'/',
        tes_dir = expand(config['data']['spikes']['lapa_ends'], end_mode='tes', allow_missing=True)[0].rsplit('/', maxsplit=1)[0]+'/'
    output:
        links = config['data']['spikes']['lapa_links']

use rule lapa_correct_talon as spike_lapa_correct_talon with:
    input:
        gtf = config['data']['spikes']['filt_gtf'],
        ab = config['data']['spikes']['filt_ab'],
        annot = config['data']['spikes']['read_annot'],
        links = config['data']['spikes']['lapa_links']
    output:
        gtf = config['data']['spikes']['lapa_gtf'],
        ab = config['data']['spikes']['lapa_ab']

use rule update_lapa_gtf as spike_update_lapa_gtf with:
    input:
        lapa_gtf = config['data']['spikes']['lapa_gtf']
    output:
        nov_gtf = config['data']['spikes']['lapa_nov_gtf']

use rule update_lapa_ab as spike_update_lapa_ab with:
    input:
        lapa_ab = config['data']['spikes']['lapa_ab']
    output:
        nov_ab = config['data']['spikes']['lapa_nov_ab']

use rule filter_lapa_ab as spike_filter_lapa_ab with:
    input:
        nov_ab = config['data']['spikes']['lapa_nov_ab']
    output:
        filt_ab = config['data']['spikes']['lapa_filt_ab']

use rule filter_lapa_gtf as spike_filter_lapa_gtf with:
    input:
        nov_gtf = config['data']['spikes']['lapa_nov_gtf'],
        nov_ab = config['data']['spikes']['lapa_nov_ab']
    params:
        filter_spikes = False,
        novs = ['Known', 'NIC', 'NNC', 'ISM_rescue']
    output:
        filt_gtf = config['data']['spikes']['lapa_filt_gtf']
