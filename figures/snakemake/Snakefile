import pandas as pd
import cerberus
import warnings
import os
import sys
from encoded_client.encoded import ENCODED

p = os.path.dirname(os.getcwd())
sys.path.append(p)

from scripts.utils import *
from scripts.plotting import *
from scripts.mane_utils import *

warnings.filterwarnings('ignore')

configfile: 'snakemake/config.yml'

species = ['human', 'mouse']
# species = ['human']

# dl_files = ['ab', 'filt_ab', 'read_annot']
dl_files = ['ab', 'filt_ab']
events = ['A3', 'A5', 'AF', 'AL', 'MX', 'RI', 'SE']

def get_mirna_ids(config):
    df = pd.read_csv(expand(config['data']['mirna']['files'],
                species='human')[0],
                sep='\t',
                header=None)
    df.columns = ['link']

    # remove first line
    df = df.loc[~df.link.str.contains('metadata')]

    df['id'] = df.link.str.rsplit('/', expand=True, n=1)[1]
    df['id'] = df['id'].str.split('.tsv', expand=True, n=1)[0]
    return df['id'].tolist()

def get_lr_ids(file_format='bam'):
    metadata = get_lr_exp_meta()
    if file_format == 'bam':
        metadata = metadata.loc[metadata.output_type=='unfiltered alignments']
    elif file_format == 'fastq':
        metadata = metadata.loc[metadata.output_type=='reads']

    return metadata.file.tolist()

rule all:
    input:
        # expand(config['data']['ab'], species=species),
        # expand(config['data']['talon_filt_ab'], species=species),
        # expand(config['data']['lapa_filt_ab'], species=species),
        # expand(config['data']['lapa_gtf'], species=species),
        # expand(config['data']['filt_ab'], species=species),
        # expand(config['data']['cerb_annot'], species=species),
        # expand(config['data']['cerb_gtf'], species=species),
        # expand(config['ref']['new_gencode_gtf'], species=species),
        # expand(config['ref']['cerberus']['gtf'], species=species),
        # expand(config['ref']['cerberus']['t_info'], species=species),
        # expand(config['ref']['cerberus']['g_info'], species=species), #,
        # expand(config['data']['t_info'], species=species),
        # expand(config['data']['major_isos'], species=species, obs_col='sample'),
        # expand(config['data']['exp_gene_subset'], species='human', obs_col=['sample', 'dataset']),
        # expand(config['data']['exp_gene_subset'], species='mouse', obs_col=['sample']),
        # expand(config['data']['pi_tpm']['tss'], species='human', obs_col=['sample', 'dataset']),
        # expand(config['data']['pi_tpm']['ic'], species='human', obs_col=['sample', 'dataset']),
        # expand(config['data']['pi_tpm']['tes'], species='human', obs_col=['sample', 'dataset']),
        # expand(config['data']['pi_tpm']['triplet'], species='human', obs_col=['sample', 'dataset']),
        # expand(config['data']['pi_tpm']['triplet'], species='mouse', obs_col=['sample']),
        # expand(config['ref']['t_info'], species=species),
        # expand(config['ref']['g_info'], species=species),
        # expand(config['data']['sg'], species=species),
        # expand(config['data']['mirna']['tsv'], species='human', encid=get_mirna_ids(config)),
        # expand(config['data']['bam'], species='human', encid=get_lr_ids('bam')),
        # expand(config['data']['fastq_gz'], species='human', encid=get_lr_ids('fastq')),
        # expand(config['data']['read_len_meta'], species='human'),
        # expand(config['data']['suppa']['psi'],
        #         event=events,
        #         species='human'),
        # expand(config['data']['suppa']['gtex']['events']['A3'],
        #                 species='human'),
        # expand(config['data']['suppa']['gtex']['cerb_ids'],
        #        species='human'),
        # expand(config['data']['psi'],
        #        species='human'),
        # expand(config['data']['suppa']['matching_events'],
        #        species='human',
        #        event=['AF', 'AL'])
        # expand(config['data']['p_pred']['summary'], species=species),
        # expand(config['data']['p_pred']['blastp_6'], species=species)
        expand(config['data']['read_len_meta'], species=species),
        config['ref']['spikes']['spike_gtf']

rule dl:
    resources:
        mem_gb = 4,
        threads = 1
    shell:
        "wget -O {output.out} {params.link}"

rule gunzip:
    resources:
        mem_gb = 4,
        threads = 1
    shell:
        "gunzip -c {input.gz} > {output.out}"

################################################################################
########################### Data download ######################################
################################################################################

use rule dl as dl_ab with:
  params:
    link = lambda w:config['encode'][w.species]['ab']
  output:
    out = config['data']['ab']

use rule dl as dl_filt_ab with:
  params:
    link = lambda w:config['encode'][w.species]['filt_ab']
  output:
    out = config['data']['filt_ab']

use rule dl as dl_read_annot with:
  params:
    link = lambda w:config['encode'][w.species]['read_annot']
  output:
    out = config['data']['read_annot']

use rule dl as dl_cerb_annot with:
  params:
    link = lambda w:config['encode'][w.species]['cerb_annot']
  output:
    out = config['data']['cerb_annot']

use rule dl as dl_ref_genome with:
  params:
    link = lambda w:config['encode'][w.species]['fa_gz']
  output:
    out = config['ref']['fa_gz']

use rule gunzip as gunzip_ref_genome with:
  input:
    gz = config['ref']['fa_gz']
  output:
    out = config['ref']['fa']

use rule dl as dl_gencode_gtf with:
  params:
    link = lambda w:config['encode'][w.species]['new_gencode_gtf_gz']
  output:
    out = config['ref']['new_gencode_gtf_gz']

use rule gunzip as gunzip_gencode with:
  input:
    gz = config['ref']['new_gencode_gtf_gz']
  output:
    out = config['ref']['new_gencode_gtf']

use rule dl as dl_gencode_fa with:
  params:
    link = lambda w:config['link'][w.species]['p_fa']
  output:
    out = config['ref']['p_fa_gz']

use rule gunzip as gunzip_gencode_fa with:
  input:
    gz = config['ref']['p_fa_gz']
  output:
    out = config['ref']['p_fa']

rule fix_gencode_fa_headers:
    input:
        fa = config['ref']['p_fa']
    resources:
        threads = 1,
        mem_gb = 4
    output:
        fa = config['ref']['p_fa_header']
    run:
        fix_fa_headers(input.fa, output.fa)

use rule dl as dl_cerb_gtf with:
  params:
    link = lambda w:config['encode'][w.species]['cerb_gtf_gz']
  output:
    out = config['data']['cerb_gtf_gz']

use rule gunzip as gunzip_cerb with:
  input:
    gz = config['data']['cerb_gtf_gz']
  output:
    out = config['data']['cerb_gtf']

use rule dl as dl_lapa_gtf with:
  params:
    link = lambda w:config['encode'][w.species]['lapa_gtf_gz']
  output:
    out = config['data']['lapa_gtf_gz']

use rule gunzip as gunzip_lapa with:
  input:
    gz = config['data']['lapa_gtf_gz']
  output:
    out = config['data']['lapa_gtf']

use rule dl as dl_talon_filt_ab with:
  params:
    link = lambda w:config['encode'][w.species]['talon_filt_ab']
  output:
    out = config['data']['talon_filt_ab']

use rule dl as dl_lapa_filt_ab with:
  params:
    link = lambda w:config['encode'][w.species]['lapa_filt_ab']
  output:
    out = config['data']['lapa_filt_ab']

rule get_chrom_sizes:
    resources:
        threads = 1,
        mem_gb = 8
    shell:
        """
        faidx {input.fa} -i chromsizes > {output.chrom_sizes}
        """

################################################################################
######################### Spike download / processing ##########################
################################################################################

use rule dl as dl_sirv_gtf with:
  params:
    link = config['encode']['spikes']['sirv_gtf_gz']
  output:
    out = config['ref']['spikes']['sirv_gtf_gz']

use rule gunzip as gunzip_sirv_gtf with:
  input:
    gz = config['ref']['spikes']['sirv_gtf_gz']
  output:
    out = config['ref']['spikes']['sirv_gtf']

use rule dl as dl_sirv_fa with:
  params:
    link = config['encode']['spikes']['sirv_fa_gz']
  output:
    out = config['ref']['spikes']['sirv_fa_gz']

use rule gunzip as gunzip_sirv_fa with:
  input:
    gz = config['ref']['spikes']['sirv_fa_gz']
  output:
    out = config['ref']['spikes']['sirv_fa']

use rule dl as dl_ercc_fa with:
  params:
    link = config['encode']['spikes']['ercc_fa_gz']
  output:
    out = config['ref']['spikes']['ercc_fa_gz']

use rule gunzip as gunzip_ercc_fa with:
  input:
    gz = config['ref']['spikes']['ercc_fa_gz']
  output:
    out = config['ref']['spikes']['ercc_fa']

rule get_ercc_gtf:
    input:
        fa = config['ref']['spikes']['ercc_fa']
    output:
        gtf = config['ref']['spikes']['ercc_gtf']
    shell:
        """
        python ../scripts/merge_encode_annotations.py \
            -o temp_reformat_ercc.gtf \
            {input.fa}

        python ../scripts/reformat_gtf.py \
            -o {output.gtf} \
            -gtf temp_reformat_ercc.gtf

        rm temp_reformat_ercc.gtf
        """

rule get_spike_gtf:
    input:
        sirv = config['ref']['spikes']['sirv_gtf'],
        ercc = config['ref']['spikes']['ercc_gtf']
    output:
        all = config['ref']['spikes']['spike_gtf']
    shell:
        """
        cat {input.sirv} > {output.all}
        cat {input.ercc} >> {output.all}
        """

rule get_spike_fa:
    input:
        sirv = config['ref']['spikes']['sirv_fa'],
        ercc = config['ref']['spikes']['ercc_fa']
    output:
        all = config['ref']['spikes']['spike_fa']
    shell:
        """
        cat {input.sirv} > {output.all}
        cat {input.ercc} >> {output.all}
        """

use rule get_chrom_sizes as get_spike_chrom_sizes with:
    input:
        fa = config['ref']['spikes']['spike_fa']
    output:
        chrom_sizes = config['ref']['spikes']['chrom_sizes']

rule get_spike_chrom_bed:
    input:
        chrom_sizes = config['ref']['spikes']['chrom_sizes']
    output:
        bed = config['ref']['spikes']['chrom_bed']
    run:
        df = pd.read_csv(input.chrom_sizes,
                         sep='\t',
                         header=None,
                         names=['chr', 'stop'])
        df['start'] = 0
        df = df[['chr', 'start', 'stop']]
        df.to_csv(output.bed,
                  sep='\t',
                  header=None,
                  index=False)

################################################################################
############################# Data processing ##################################
################################################################################
def get_new_gencode_source(wildcards):
    if wildcards.species == 'human':
        source = 'v40'
    elif wildcards.species == 'mouse':
        source = 'vM25'
    return source

# annotate gencode gtf w/ cerberus
rule cerberus_update_gtf:
    input:
        annot = config['data']['cerb_annot'],
        gtf = config['ref']['new_gencode_gtf']
    output:
        update_gtf = config['ref']['cerberus']['gtf']
    resources:
        mem_gb = 56,
        threads = 1
    params:
        source = get_new_gencode_source
    shell:
        "cerberus replace_gtf_ids \
            --h5 {input.annot} \
            --gtf {input.gtf} \
            --source {params.source} \
            --update_ends \
            --collapse \
            -o {output.update_gtf}"

# get transcript info
rule get_t_info:
    resources:
        mem_gb = 56,
        threads = 1
    run:
        get_transcript_info(input.gtf, output.o)

use rule get_t_info as ref_cerb_t_info with:
    input:
        gtf = config['ref']['cerberus']['gtf']
    output:
        o = config['ref']['cerberus']['t_info']

use rule get_t_info as ref_t_info with:
    input:
        gtf = config['ref']['new_gencode_gtf']
    output:
        o = config['ref']['t_info']

use rule get_t_info as cerb_t_info with:
    input:
        gtf = config['data']['cerb_gtf']
    output:
        o = config['data']['t_info']

# get gene info
rule get_g_info:
    resources:
        mem_gb = 56,
        threads = 1
    run:
        get_gene_info(input.gtf, output.o)

use rule get_g_info as cerb_ref_g_info with:
    input:
        gtf = config['ref']['cerberus']['gtf']
    output:
        o = config['ref']['cerberus']['g_info']

use rule get_g_info as ref_g_info with:
    input:
        gtf = config['ref']['new_gencode_gtf']
    output:
        o = config['ref']['g_info']

################################################################################
################################## Swan ########################################
################################################################################
def make_sg(input, params, wildcards):

    # initialize
    sg = swan.SwanGraph()
    sg.add_annotation(input.annot)
    sg.add_transcriptome(input.gtf, include_isms=True)
    sg.add_abundance(input.ab)
    sg.add_abundance(input.gene_ab, how='gene')
    sg.save_graph(params.prefix)

    # add metadata and add colors
    sg.add_metadata(input.meta)
    c_dict, order = get_biosample_colors(wildcards.species)
    sg.set_metadata_colors('sample', c_dict)

    # human only settings
    if wildcards.species == 'human':
        c_dict, order = get_ad_colors()
        sg.set_metadata_colors('health_status', c_dict)
    # save
    sg.save_graph(params.prefix)

rule swan_init:
    input:
        annot = config['ref']['cerberus']['gtf'],
        ab = config['data']['filt_ab'],
        gene_ab = config['data']['ab'],
        gtf = config['data']['cerb_gtf'],
        meta = config['data']['meta']
    params:
        prefix = config['data']['sg'].replace('.p', '')
    resources:
        mem_gb = 64,
        threads = 1
    output:
        sg = config['data']['sg']
    run:
        make_sg(input, params, wildcards)

rule major_isos:
    input:
        sg = config['data']['sg'],
        filt_ab = config['data']['filt_ab']
    resources:
        mem_gb = 16,
        threads = 8
    params:
        min_tpm = 1,
        gene_subset = 'polya'
    output:
        ofile = config['data']['major_isos']
    run:
        get_major_isos(input.sg,
                       input.filt_ab,
                       wildcards.obs_col,
                       output.ofile,
                       min_tpm=params.min_tpm,
                       gene_subset=params.gene_subset)

################################################################################
################################## MANE ########################################
################################################################################
rule get_exp_genes:
    input:
        ab = config['data']['ab']
    resources:
        mem_gb = 16,
        threads = 8
    params:
        min_tpm = 1,
        gene_subset = 'polya',
    output:
        ofile = config['data']['exp_gene_subset']
    run:
        get_exp_gene_subset(input.ab,
                            params.min_tpm,
                            wildcards.obs_col,
                            output.ofile,
                            wildcards.species)

rule get_pi_tpm:
    input:
        swan_file = config['data']['sg']
    resources:
        mem_gb = 32,
        threads = 8
    params:
        odir = config['data']['pi_tpm']['tss'].rsplit('/', maxsplit=1)[0]
    output:
        config['data']['pi_tpm']['tss'],
        config['data']['pi_tpm']['ic'],
        config['data']['pi_tpm']['tes'],
        config['data']['pi_tpm']['triplet']
    run:
        sg = swan.read(input.swan_file)
        get_pi_tpm_tables(sg, wildcards.obs_col, params.odir)

################################################################################
################################## miRNA #######################################
################################################################################

rule dl_mirna:
    resources:
        mem_gb = 32,
        threads = 8
    output:
        tsv = config['data']['mirna']['tsv']
    shell:
        "wget https://www.encodeproject.org/files/{wildcards.encid}/@@download/{wildcards.encid}.tsv -O {output.tsv}"

################################################################################
############################## Diane's stuff ###################################
################################################################################
rule dl_lr_bam:
    resources:
        mem_gb = 32,
        threads = 8
    output:
        bam = config['data']['bam']
    shell:
        "wget https://www.encodeproject.org/files/{wildcards.encid}/@@download/{wildcards.encid}.bam -O {output.bam}"

rule dl_lr_fastq:
    resources:
        mem_gb = 32,
        threads = 8
    output:
        fastq = config['data']['fastq_gz']
    shell:
        "wget https://www.encodeproject.org/files/{wildcards.encid}/@@download/{wildcards.encid}.fastq.gz -O {output.fastq}"

rule get_lr_read_lens:
    input:
        bams = expand(config['data']['bam'], species='human', encid=get_lr_ids('bam')),
        fastqs = expand(config['data']['fastq_gz'], species='human', encid=get_lr_ids('fastq'))
    resources:
        mem_gb = 32,
        threads = 8
    output:
        tsv = config['data']['read_len_meta']
    run:
        get_lr_read_lens(input.bams, input.fastqs, output.tsv)


################################################################################
################################# SUPPA ########################################
################################################################################

use rule dl as dl_gtex_gtf with:
  params:
    link = lambda w:config['encode'][w.species]['gtex_gtf_gz']
  output:
    out = config['ref']['gtex_gtf_gz']

use rule gunzip as gunzip_gtex with:
  input:
    gz = config['ref']['gtex_gtf_gz']
  output:
    out = config['ref']['gtex_gtf']

rule get_transcript_novelties:
    input:
        annot = config['data']['cerb_annot'],
        filt_ab = config['data']['filt_ab'],
        t_meta = config['data']['t_info']
    params:
        min_tpm = 1,
        gene_subset = 'polya',
        ver = 'v40_cereberus'
    resources:
        mem_gb = 8,
        threads = 1
    output:
        novelties = config['data']['novelties']
    run:
        get_transcript_novelties(input.annot,
                                 input.filt_ab,
                                 input.t_meta,
                                 params.min_tpm,
                                 params.gene_subset,
                                 params.ver,
                                 output.novelties)

rule preproc_suppa:
    input:
        gtf = config['data']['cerb_gtf'],
        nov = config['data']['novelties'],
        filt_ab = config['data']['filt_ab']
    params:
        temp_filt_ab_nov = 'temp_filt_ab_nov.tsv',
        temp_filt_ab_nov_2 = 'temp_filt_ab_nov_2.tsv'
    output:
        filt_ab = config['data']['suppa']['filt_ab'],
        gtf = config['data']['suppa']['gtf']
    conda:
        'seurat'
    shell:
        """
        sed 's/,/_/g' {input.gtf} > {output.gtf}
        Rscript ../scripts/filter_abundance_mat_by_novelty.R \
            --novelties {input.nov} \
            --filtab {input.filt_ab} \
            --ofile {params.temp_filt_ab_nov}
        cut -f4,12- {params.temp_filt_ab_nov} | sed 's/,/_/g' > {params.temp_filt_ab_nov_2}
        python ../scripts/suppa_format_rm_colname.py {params.temp_filt_ab_nov_2}
        tail -c +2 {params.temp_filt_ab_nov_2} > {output.filt_ab}
        """

rule suppa_generate_events:
    resources:
        mem_gb = 16,
        threads = 1
    shell:
        """
        suppa.py generateEvents \
            -i {input.gtf} \
            -o {params.opref} \
            -f ioe \
            -e SE SS MX RI FL
        """

use rule suppa_generate_events as ge_cerb with:
    input:
        gtf = config['data']['suppa']['gtf']
    params:
        opref = config['data']['suppa']['events']['A3'].rsplit('_', maxsplit=2)[0]
        # opref = config['data']['suppa']['events']
    output:
        config['data']['suppa']['events']['A3'],
        config['data']['suppa']['events']['A5'],
        config['data']['suppa']['events']['AF'],
        config['data']['suppa']['events']['AL'],
        config['data']['suppa']['events']['MX'],
        config['data']['suppa']['events']['RI'],
        config['data']['suppa']['events']['SE'],

use rule suppa_generate_events as ge_gtex with:
    input:
        gtf = config['ref']['gtex_gtf']
    params:
        opref = config['data']['suppa']['gtex']['events']['A3'].rsplit('_', maxsplit=2)[0]
    output:
        config['data']['suppa']['gtex']['events']['A3'],
        config['data']['suppa']['gtex']['events']['A5'],
        config['data']['suppa']['gtex']['events']['AF'],
        config['data']['suppa']['gtex']['events']['AL'],
        config['data']['suppa']['gtex']['events']['MX'],
        config['data']['suppa']['gtex']['events']['RI'],
        config['data']['suppa']['gtex']['events']['SE']

rule suppa_psi:
    resources:
        mem_gb = 16,
        threads = 1
    shell:
        """
        suppa.py psiPerEvent \
            --ioe-file {input.ioe} \
            --expression-file {input.filt_ab} \
            --o {params.opref}
        """

use rule suppa_psi as psi_cerb with:
    input:
        ioe = lambda w:config['data']['suppa']['events'][w.event],
        filt_ab = config['data']['suppa']['filt_ab']
    params:
        opref = config['data']['suppa']['psi'].rsplit('.psi', maxsplit=1)[0]
    output:
        out = config['data']['suppa']['psi']

rule get_gtex_cerb_ids:
    input:
        cerb_annot = config['data']['cerb_annot']
    resources:
        mem_gb = 2,
        threads = 1
    output:
        cerb_ids = config['data']['suppa']['gtex']['cerb_ids']
    run:
        get_gtex_cerberus_ids(input.cerb_annot, output.cerb_ids)


rule get_cerberus_psi:
    input:
        filt_ab = config['data']['filt_ab']
    params:
        min_tpm = 1,
        gene_subset = 'polya'
    resources:
        threads = 1,
        mem_gb = 64
    output:
        ofile = config['data']['psi']
    run:
        get_cerberus_psi(input.filt_ab,
                         params.min_tpm,
                         params.gene_subset,
                         output.ofile)

e_map = {'tss': 'AF',
      'tes': 'AL',
      'AL': 'tes',
      'AF': 'tss'}
rule get_matching_events:
    input:
        cerb_file = config['data']['psi'],
        suppa_file = config['data']['suppa']['psi'],
        lib_meta = config['data']['meta']
    params:
        kind = lambda w:e_map[w.event]
    output:
        ofile = config['data']['suppa']['matching_events']
    run:
        get_cerb_suppa_matching_events(input.cerb_file,
                                       input.suppa_file,
                                       output.ofile,
                                       input.lib_meta,
                                       params.kind)



# use rule get_matching_events as gme_al with:
#
#
# use rule get_matching_events as gme_al with:
#     input:
#         cerb_file = config['data']['psi'],
#         suppa_file = config['data']['suppa']['psi'],
#         lib_meta = config['data']['meta'],
#     output:
#         ofile = config['data']['suppa']['matching_events']

################################################################################
######################### Milad's TSS prediction ###############################
################################################################################

# TODO

# automate downloading the bigwigs, getting the processed tss data from cerberus
# and lapa, and downloading the jamboree tss files

################################################################################
##################### Narges's short-read data proc ############################
################################################################################

# TODO

################################################################################
######################## Sam's gene expression #################################
################################################################################

# TODO

################################################################################
################################ Protein prediction ############################
################################################################################

rule tama_gtf_to_bed:
    input:
        gtf = config['data']['cerb_gtf']
    params:
        tama_dir = config['tama_dir']
    resources:
        threads = 4,
        mem_gb = 32
    output:
        bed = config['data']['p_pred']['gtf_bed']
    shell:
        """
        python {params.tama_dir}/format_converter/tama_format_gtf_to_bed12_ensembl.py \
          {input.gtf} \
          {output.bed}
        """

rule bed_to_fasta:
    input:
        bed = config['data']['p_pred']['gtf_bed'],
        ref = config['ref']['fa']
    output:
        fa = config['data']['p_pred']['fa']
    resources:
        threads = 4,
        mem_gb = 32
    shell:
        """
        # module load bedtools2
        bedtools getfasta \
            -name \
            -split \
            -s \
            -fi {input.ref} \
            -bed {input.bed} \
            -fo {output.fa}
        """

rule tama_orf:
    input:
        fa = config['data']['p_pred']['fa']
    params:
        tama_dir = config['tama_dir']
    resources:
        threads = 4,
        mem_gb = 32
    output:
        orf_fa = config['data']['p_pred']['orf_fa']
    shell:
        """
        python {params.tama_dir}/orf_nmd_predictions/tama_orf_seeker.py \
            -f {input.fa} \
            -o {output.orf_fa}
        """

rule make_blastp_db:
    input:
        fa = config['ref']['p_fa_header']
    resources:
        mem_gb = 64,
        threads = 32
    params:
        opref = config['ref']['p_db'].replace('.pdb', '')

    output:
        db = config['ref']['p_db']
    shell:
        """
        makeblastdb \
            -in {input.fa} \
             -dbtype prot \
             -parse_seqids \
             -out {params.opref}
        """

# rule blast:
#     input:
#         orf_fa = config['data']['p_pred']['orf_fa'],
#         db = config['ref']['p_db']
#     params:
#         opref = config['ref']['p_db'].replace('.pdb', '')
#     resources:
#         mem_gb = 32,
#         threads = 32
#     output:
#         out = config['data']['p_pred']['blastp']
#     shell:
#         """
#          blastp \
#           -evalue 1e-10 \
#           -num_threads 32 \
#           -db {params.opref} \
#           -outfmt 0 \
#           -query {input.orf_fa} > \
#           {output.out}
#         """

rule blast_6:
    input:
        orf_fa = config['data']['p_pred']['orf_fa'],
        db = config['ref']['p_db']
    params:
        opref = config['ref']['p_db'].replace('.pdb', '')
    resources:
        mem_gb = 32,
        threads = 32
    output:
        out = config['data']['p_pred']['blastp_6']
    shell:
        """
         blastp \
          -evalue 1e-10 \
          -num_threads 32 \
          -db {params.opref} \
          -outfmt "6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore slen qlen" \
          -query {input.orf_fa} > \
          {output.out}
        """

# rule parse_blastp:
#     input:
#         blastp = config['data']['p_pred']['blastp_6']
#     params:
#         tama_dir = config['tama_dir']
#     resources:
#         mem_gb = 32,
#         threads = 1
#     output:
#         parsed = config['data']['p_pred']['blastp_parsed']
#     shell:
#         """
#         python {params.tama_dir}/orf_nmd_predictions/tama_orf_blastp_parser.py \
#           -b {input.blastp} \
#           -o {output.parsed}
#         """

rule parse_blastp:
    input:
        blastp = config['data']['p_pred']['blastp_6'],
        fa = config['data']['p_pred']['orf_fa']
    resources:
        mem_gb = 32,
        threads = 1
    output:
        parsed = config['data']['p_pred']['blastp_parsed']
    run:
        parse_blastp(input.blastp, input.fa, output.parsed)


rule tama_cds_bed_regions:
    input:
        parsed = config['data']['p_pred']['blastp_parsed'],
        gtf_bed = config['data']['p_pred']['gtf_bed'],
        fa = config['data']['p_pred']['fa']
    params:
        tama_dir = config['tama_dir']
    resources:
        mem_gb = 4,
        threads = 1
    output:
        bed = config['data']['p_pred']['cds_bed']
    shell:
        """
        python {params.tama_dir}/orf_nmd_predictions/tama_cds_regions_bed_add.py \
            -p {input.parsed} \
            -a {input.gtf_bed} \
            -f {input.fa} \
            -o {output.bed}
        """

rule get_pp_summary:
    input:
        fa = config['data']['p_pred']['orf_fa'],
        parsed = config['data']['p_pred']['blastp_parsed'],
        cds = config['data']['p_pred']['cds_bed']
    resources:
        mem_gb = 4,
        threads = 1
    output:
        summary = config['data']['p_pred']['summary']
    run:
        get_pp_info(input.fa,
                    input.cds,
                    input.parsed,
                    output.summary)


################################################################################
############################## Spike data proc #################################
################################################################################

rule get_spike_reads:
    input:
        bam = config['data']['bam'],
        spike_bed = config['ref']['spikes']['chrom_bed']
    output:
        bam = config['data']['spike']['bam']
    shell:
        """
        module load samtools
        samtools index {input.bam}
        samtools view -bh --regions-file {input.spike_bed} > {output.bam}
        """
