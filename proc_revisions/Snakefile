# TODO add that daataset that you're missing from mouse to the cart!!!
# TODO rename "adrenal gland" to "adrenal_gland"
import pandas as pd
import os
import sys
import numpy as np

p = os.getcwd()
sys.path.append(p)

from sm_utils import *

include: 'download.smk'
include: 'samtools.smk'
include: 'spike_refs.smk'
include: 'refs.smk'
include: 'talon.smk'

############# config stuff
configfile: 'config.yml'

datasets_per_talon_run = 14
species=['human', 'mouse']

# lr stuff
# lr_df = pd.read_csv(expand(config['lr']['meta'], species=species)[0], sep='\t')
lr_df = process_lr_metadata(config['lr']['meta'], ['human', 'mouse'],
                            datasets_per_talon_run)

# need to generalize this for multiple species
# lr_meta = process_encode_metadata(expand(config['lr']['encode_meta'], species=species)[0])

wildcard_constraints:
    dataset='|'.join([re.escape(x) for x in lr_df.dataset.tolist()]),
    species='|'.join([re.escape(x) for x in lr_df.species.unique().tolist()])

# ruleorder:
#     first_talon > seq_talon


############ rules
rule all:
    input:
        expand(config['lr']['talon']['bam_ind'],
               zip,
               species=lr_df.species.tolist(),
               dataset=lr_df.dataset.tolist()),
        expand(config['lr']['talon']['config'],
               zip,
               species=lr_df.species.tolist(),
               talon_run=lr_df.talon_run.tolist())
        # expand(config['lr']['bam_ind'],
        #        species=species,
        #        dataset=['hl60_1_1',
        #                 'brodmann_area_46_1_1'])



################################################################################
############################### LR Download ####################################
################################################################################
def get_lr_encid(wc, df):
    dataset = wc.dataset
    species = wc.species
    temp = df.loc[(df.dataset==dataset)&\
                  (df.species==species)]
    return temp['ENCODE_alignments_id'].values[0]

use rule dl_encid as lr_dl with:
    params:
        encid = lambda wc:get_lr_encid(wc, lr_df),
        file_ext = 'bam'
    output:
        out = temporary(config['lr']['bam'])
################################################################################
#################################### TALON #####################################
################################################################################

# label reads and create indexed BAM files to retain
rule talon_label:
    input:
        bam = config['lr']['bam'],
        fa = config['ref']['talon']['fa']
    resources:
        mem_gb = 64,
        threads = 1
    params:
        opref = config['lr']['talon']['sam_label'].rsplit('_labeled.sam')[0]
    output:
        bam = temporary(config['lr']['talon']['sam_label']),
        tsv = temporary(config['lr']['talon']['read_labels'])
    shell:
        """
        talon_label_reads \
            --f {input.bam} \
            --g {input.fa} \
            --tmpDir {params.opref}_tmp/ \
            --ar 20  \
            --deleteTmp  \
            --o {params.opref}
        """

use rule sam_to_bam as lr_sam_to_bam with:
    input:
        sam = config['lr']['talon']['sam_label']
    output:
        bam = temporary(config['lr']['talon']['bam'])

use rule sort_bam as lr_sort_bam with:
    input:
        bam = config['lr']['talon']['bam']
    output:
        bam = config['lr']['talon']['bam_sort']

use rule index_bam as lr_index_bam with:
    input:
        bam = config['lr']['talon']['bam_sort']
    output:
        ind = config['lr']['talon']['bam_ind']

# initialize TALON db
use rule talon_init as talon_init_db with:
    input:
        gtf = config['ref']['talon']['gtf']
    params:
        genome_ver = lambda wc:config['ref'][wc.species]['fa_ver'],
        annot_ver = lambda wc:config['ref'][wc.species]['gtf_ver'],
        opref = config['lr']['talon']['ref_db'].rsplit('.db', maxsplit=1)[0]

# get config files for each talon run
rule talon_config:
    input:
        files = lambda wc:get_talon_run_info(wc, lr_df,
                            cfg_entry=config['lr']['talon']['bam_sort'],
                            files=True)
    resources:
        threads = 1,
        mem_gb = 1
    params:
        df = lr_df
    output:
        config = config['lr']['talon']['config']
    run:
        cfg_df = get_talon_run_info(wildcards, params.df,
                                    config['lr']['talon']['bam_sort'],
                                    files=False)
        import pdb; pdb.set_trace()
        cfg_df = cfg_df[['dataset', 'sample', 'platform', 'talon_file']]
        cfg_df.to_csv(output.config, header=None, sep=',', index=False)

# first talon run
# use rule talon as first_talon with:
#     input:
#         ref = config['lr']['talon']['ref_db']
#     params:
#         genome_ver = lambda wc:config['ref'][wc.species]['fa_ver'],
#         opref = config['']
#     output:
#         db = expand(config['lr']['talon']['db'],
#                     zip,
#                     talon_run=0,
#                     allow_missing=True)


# sequential talon runs
